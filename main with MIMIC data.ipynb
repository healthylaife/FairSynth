{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf427c-d9d2-4908-8525-f0e75588ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader\n",
    "from pyhealth.models import Transformer,RNN\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from pyhealth.metrics.fairness import fairness_metrics_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fe316-bed7-4814-8497-0c1f8ceef813",
   "metadata": {},
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d34d2-3a85-4012-bc6f-82ec20b9bbc6",
   "metadata": {},
   "source": [
    "This code works with the public MIMIC-III ICU stay database. Before using the code, please apply, complete training, and download the requisite files from <https://physionet.org>. The required files are:\n",
    "* 'PATIENTS.csv'\n",
    "* 'ADMISSIONS.csv'\n",
    "* 'DIAGNOSES_ICD.csv'\n",
    "* 'PROCEDURES_ICD.csv'\n",
    "* 'PRESCRIPTIONS.csv'\n",
    "* 'CHARTEVENTS.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0313630-0e45-4136-8c70-5fecfd169702",
   "metadata": {},
   "source": [
    "## Patient Event Generation\n",
    "Next, to create the patient events you need to perform the mimic3-benchmarks preprocessing according the the repository found at <https://github.com/YerevaNN/mimic3-benchmarks>. That repository has comprehensive documentation, and it will create a series of .csv files containing lab time-series information for each ICU stay. You just need to get through the `extract_episodes_from_subjects` step.\n",
    "\n",
    "From there,  edit the `mimic_dir` and `timeseries_dir` variables in the notebook. Running the cell will generate all of the base data files for the experiments.\n",
    "\n",
    "Next, we need to discretize the continuous variables to feed them into the generator. To do so, create a `discretized_data/` directory and run the cell under 'discretize'\n",
    "\n",
    "At this point, the discretized data and corresponding artifacts will be available, and your dataset will be fully processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7d675-ed0a-49cb-adbb-b1ee7bdf14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_dir = \"D:\\\\Phd\\\\Prelim\\\\mimic3\\\\\"\n",
    "timeseries_dir = \"data\\\\mimic3-benchmarks\\\\root\\\\\"\n",
    "valid_subjects = os.listdir(timeseries_dir)\n",
    "patientsFile = mimic_dir + \"PATIENTS.csv\"\n",
    "admissionFile = mimic_dir + \"ADMISSIONS.csv\"\n",
    "diagnosisFile = mimic_dir + \"DIAGNOSES_ICD.csv\"\n",
    "procedureFile = mimic_dir + \"PROCEDURES_ICD.csv\"\n",
    "medicationFile = mimic_dir + \"PRESCRIPTIONS.csv\"\n",
    "\n",
    "channel_to_id = pickle.load(open(\"data\\\\channel_to_id.pkl\", \"rb\"))\n",
    "is_categorical_channel = pickle.load(open(\"data\\\\is_categorical_channel.pkl\", \"rb\"))\n",
    "possible_values = pickle.load(open(\"data\\\\possible_values.pkl\", \"rb\"))\n",
    "begin_pos = pickle.load(open(\"data\\\\begin_pos.pkl\", \"rb\"))\n",
    "end_pos = pickle.load(open(\"data\\\\end_pos.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Loading CSVs Into Dataframes\")\n",
    "patientsDf = pd.read_csv(patientsFile, dtype=str).set_index(\"SUBJECT_ID\")\n",
    "patientsDf['DOB'] = pd.to_datetime(patientsDf['DOB'])\n",
    "patientsDf['DOD'] = pd.to_datetime(patientsDf['DOD'])\n",
    "patientsDf['DOD_HOSP'] = pd.to_datetime(patientsDf['DOD_HOSP'])\n",
    "\n",
    "gender_mapping = {gender: index for index, gender in enumerate({'M','F'})}\n",
    "patientsDf['GENDER_MAP'] = patientsDf['GENDER'].map(gender_mapping)\n",
    "\n",
    "admissionDf = pd.read_csv(admissionFile, dtype=str)\n",
    "admissionDf['ADMITTIME'] = pd.to_datetime(admissionDf['ADMITTIME'])\n",
    "admissionDf['DISCHTIME'] = pd.to_datetime(admissionDf['DISCHTIME'])\n",
    "admissionDf['DEATHTIME'] = pd.to_datetime(admissionDf['DEATHTIME'])\n",
    "def map_ethnicity(ethnicity):\n",
    "    if 'WHITE' in ethnicity:\n",
    "        return 0\n",
    "    elif 'BLACK' in ethnicity:\n",
    "        return 1\n",
    "    elif 'HISPANIC' in ethnicity:\n",
    "        return 2\n",
    "    elif 'ASIAN' in ethnicity:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "admissionDf[\"ETHNICITY_MAP\"] = admissionDf['ETHNICITY'].apply(map_ethnicity)\n",
    "insurance_mapping = {\n",
    "    'Private': 0,\n",
    "    'Medicare': 1,\n",
    "    'Medicaid': 2,\n",
    "    'Self Pay': 3,\n",
    "    'Government': 4\n",
    "}\n",
    "admissionDf[\"INSURANCE_MAP\"] = admissionDf['INSURANCE'].map(insurance_mapping)\n",
    "admissionDf = admissionDf.sort_values('ADMITTIME')\n",
    "admissionDf = admissionDf.reset_index(drop=True)\n",
    "\n",
    "diagnosisDf = pd.read_csv(diagnosisFile, dtype=str).set_index(\"HADM_ID\")\n",
    "diagnosisDf = diagnosisDf[diagnosisDf['ICD9_CODE'].notnull()]\n",
    "diagnosisDf = diagnosisDf[diagnosisDf['ICD9_CODE'].str.startswith(\"428\")]\n",
    "diagnosisDf = diagnosisDf[['ICD9_CODE']]\n",
    "\n",
    "procedureDf = pd.read_csv(procedureFile, dtype=str).set_index(\"HADM_ID\")\n",
    "procedureDf = procedureDf[procedureDf['ICD9_CODE'].notnull()]\n",
    "procedureDf = procedureDf[['ICD9_CODE']]\n",
    "\n",
    "medicationDf = pd.read_csv(medicationFile, dtype=str).set_index(\"HADM_ID\")\n",
    "medicationDf = medicationDf[medicationDf['NDC'].notnull()]\n",
    "medicationDf = medicationDf[medicationDf['NDC'] != 0]\n",
    "medicationDf = medicationDf[['NDC', 'DRUG']]\n",
    "medicationDf['NDC'] = medicationDf['NDC'].astype(np.int64).astype(str)\n",
    "medicationDf['NDC'] = [('0' * (11 - len(c))) + c for c in medicationDf['NDC']]\n",
    "medicationDf['NDC'] = [c[0:5] + '-' + c[5:9] + '-' + c[10:12] for c in medicationDf['NDC']]\n",
    "print(\"DONE LAODING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30910-5a1e-4f1f-8445-40e3cd3317c4",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9ff1c-0789-4ec6-a051-15535c0c5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming patientDf and admissionDf are your dataframes\n",
    "merged_data = pd.merge(admissionDf, patientsDf, on='SUBJECT_ID')\n",
    "\n",
    "# Gender distribution\n",
    "gender_counts = merged_data['GENDER_MAP'].value_counts()\n",
    "gender_deaths = merged_data.groupby(['GENDER_MAP', 'HOSPITAL_EXPIRE_FLAG']).size().unstack(fill_value=0)\n",
    "\n",
    "# Racial distribution\n",
    "race_counts = merged_data['ETHNICITY_MAP'].value_counts()\n",
    "race_deaths = merged_data.groupby(['ETHNICITY_MAP', 'HOSPITAL_EXPIRE_FLAG']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create 2x2 subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Gender distribution\n",
    "sns.barplot(x=gender_counts.index, y=gender_counts.values, color='skyblue', ax=axs[0, 0])\n",
    "axs[0, 0].set_title('Gender Distribution')\n",
    "axs[0, 0].set_xlabel('Gender: 0 is Male and 1 is Female')\n",
    "axs[0, 0].set_ylabel('Number of Patients')\n",
    "for ax in axs.flatten():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "# Racial distribution\n",
    "sns.barplot(x=race_counts.index, y=race_counts.values, color='skyblue', ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Racial Distribution')\n",
    "axs[0, 1].set_xlabel('Race')\n",
    "axs[0, 1].set_ylabel('Number of Patients')\n",
    "for ax in axs.flatten():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "# Number of patients of each gender (stacked)\n",
    "sns.barplot(data=gender_counts.reset_index(), x='index', y='GENDER_MAP', color='skyblue', ax=axs[1, 0])\n",
    "sns.barplot(data=gender_deaths.reset_index(), x='GENDER_MAP', y=1, color='red', ax=axs[1, 0])\n",
    "sns.barplot(data=gender_deaths.reset_index(), x='GENDER_MAP', y=0, color='green', ax=axs[1, 0])\n",
    "axs[1, 0].set_title('Gender Distribution with Deaths')\n",
    "axs[1, 0].set_xlabel('Gender')\n",
    "axs[1, 0].set_ylabel('Number of Patients')\n",
    "for ax in axs.flatten():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "# Number of patients of each ethnicity (stacked)\n",
    "sns.barplot(data=race_counts.reset_index(), x='index', y='ETHNICITY_MAP', color='skyblue', ax=axs[1, 1])\n",
    "sns.barplot(data=race_deaths.reset_index(), x='ETHNICITY_MAP', y=1, color='red', ax=axs[1, 1])\n",
    "sns.barplot(data=race_deaths.reset_index(), x='ETHNICITY_MAP', y=0, color='green', ax=axs[1, 1])\n",
    "axs[1, 1].set_title('Racial Distribution with Deaths')\n",
    "axs[1, 1].set_xlabel('Race')\n",
    "axs[1, 1].set_ylabel('Number of Patients')\n",
    "for ax in axs.flatten():\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 5),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64877c9-56fd-45de-96c2-86e93032dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Smaller dataset\n",
    "print(len(admissionDf))\n",
    "# Randomly sample 10,000 rows\n",
    "admissionDf = admissionDf.sample(n=1000, random_state=42)  # You can adjust the random_state for reproducibility\n",
    "\n",
    "print(len(admissionDf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3249bd4b-628a-43e4-a659-4e707a8700bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Dataset\")\n",
    "data = {}\n",
    "for row in tqdm(admissionDf.itertuples(), total=len(admissionDf)):\n",
    "    hadm_id = row.HADM_ID\n",
    "    subject_id = row.SUBJECT_ID\n",
    "    admit_time = row.ADMITTIME\n",
    "    ethnicity = row.ETHNICITY_MAP\n",
    "    insurance = row.INSURANCE_MAP\n",
    "\n",
    "    if subject_id not in patientsDf.index:\n",
    "        continue\n",
    "    visit_count = (0 if subject_id not in data else len(data[subject_id][\"visits\"])) + 1\n",
    "\n",
    "    tsDf = (\n",
    "        pd.read_csv(f\"{timeseries_dir}{subject_id}/episode{visit_count}_timeseries.csv\")\n",
    "        if os.path.exists(\n",
    "            f\"{timeseries_dir}{subject_id}/episode{visit_count}_timeseries.csv\"\n",
    "        )\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    patientRow = patientsDf.loc[[subject_id]].iloc[0]\n",
    "    age = (admit_time.to_pydatetime() - patientRow[\"DOB\"].to_pydatetime()).days / 365\n",
    "    if age > 120:\n",
    "        continue\n",
    "\n",
    "    # Extracting the Diagnoses\n",
    "    if hadm_id in diagnosisDf.index:\n",
    "        diagnoses = list(set(diagnosisDf.loc[[hadm_id]][\"ICD9_CODE\"]))\n",
    "    else:\n",
    "        diagnoses = []\n",
    "\n",
    "    # Extracting the Procedures\n",
    "    if hadm_id in procedureDf.index:\n",
    "        procedures = list(set(procedureDf.loc[[hadm_id]][\"ICD9_CODE\"]))\n",
    "    else:\n",
    "        procedures = []\n",
    "\n",
    "    # Extracting the Medications\n",
    "    if hadm_id in medicationDf.index:\n",
    "        medications = list(set(medicationDf.loc[[hadm_id]][\"NDC\"]))\n",
    "    else:\n",
    "        medications = []\n",
    "\n",
    "    # Extract the lab timeseries\n",
    "    labs = []\n",
    "    prevTime = 0\n",
    "    currTime = int(tsDf.iloc[0][\"Hours\"]) if tsDf is not None else 0\n",
    "    currMask = []\n",
    "    currValues = []\n",
    "    if tsDf is not None:\n",
    "        for i, row in tsDf.iterrows():\n",
    "            rowTime = int(row[\"Hours\"])\n",
    "\n",
    "            if rowTime != currTime:\n",
    "                labs.append((currMask, currValues, [currTime - prevTime]))\n",
    "                prevTime = currTime\n",
    "                currTime = rowTime\n",
    "                currMask = []\n",
    "                currValues = []\n",
    "\n",
    "            for col, value in row.items():\n",
    "                if value != value or col == \"Hours\":\n",
    "                    continue\n",
    "\n",
    "                if is_categorical_channel[col]:\n",
    "                    if col == \"Glascow coma scale total\":\n",
    "                        value = str(int(value))\n",
    "                    elif col == \"Capillary refill rate\":\n",
    "                        value = str(value)\n",
    "\n",
    "                    if begin_pos[channel_to_id[col]] in currMask:\n",
    "                        currValues[\n",
    "                            currMask.index(\n",
    "                                begin_pos[channel_to_id[col]]\n",
    "                                + possible_values[col].index(value)\n",
    "                            )\n",
    "                        ] = 1\n",
    "                    else:\n",
    "                        for j in range(\n",
    "                            begin_pos[channel_to_id[col]], end_pos[channel_to_id[col]]\n",
    "                        ):\n",
    "                            currMask.append(j)\n",
    "                            currValues.append(\n",
    "                                1\n",
    "                                if j - begin_pos[channel_to_id[col]]\n",
    "                                == possible_values[col].index(value)\n",
    "                                else 0\n",
    "                            )\n",
    "                else:\n",
    "                    if begin_pos[channel_to_id[col]] in currMask:\n",
    "                        currValues[\n",
    "                            currMask.index(begin_pos[channel_to_id[col]])\n",
    "                        ] = value\n",
    "                    else:\n",
    "                        currMask.append(begin_pos[channel_to_id[col]])\n",
    "                        currValues.append(value)\n",
    "\n",
    "        labs.append((currMask, currValues, [currTime - prevTime]))\n",
    "\n",
    "        # Building the hospital admission data point\n",
    "    if subject_id not in data:\n",
    "        data[subject_id] = {\n",
    "            \"visits\": [(diagnoses, procedures, medications, age, labs)],\n",
    "            \"gender\": patientRow.GENDER_MAP,\n",
    "            \"ethnicity\": ethnicity,\n",
    "            \"insurance\": insurance,\n",
    "            \"isDead\": patientRow.EXPIRE_FLAG,\n",
    "        }\n",
    "    else:\n",
    "        data[subject_id][\"visits\"].append(\n",
    "            (diagnoses, procedures, medications, age, labs)\n",
    "        )\n",
    "pickle.dump(data, open(\"./data/data_genDatasetContinuous.pkl\", \"wb\"))\n",
    "\n",
    "# Build the label mapping\n",
    "print(\"Adding Labels\")\n",
    "with open(\"../hcup_ccs_2015_definitions_benchmark.yaml\") as definitions_file:\n",
    "    definitions = yaml.full_load(definitions_file)\n",
    "\n",
    "code_to_group = {}\n",
    "for group in definitions:\n",
    "    if definitions[group][\"use_in_benchmark\"] == False:\n",
    "        continue\n",
    "    codes = definitions[group][\"codes\"]\n",
    "    for code in codes:\n",
    "        if code not in code_to_group:\n",
    "            code_to_group[code] = group\n",
    "        else:\n",
    "            assert code_to_group[code] == group\n",
    "\n",
    "id_to_group = sorted(\n",
    "    [k for k in definitions.keys() if definitions[k][\"use_in_benchmark\"] == True]\n",
    ")\n",
    "group_to_id = dict((x, i) for (i, x) in enumerate(id_to_group))\n",
    "\n",
    "for p in data:\n",
    "    label = np.zeros(len(group_to_id))\n",
    "    for v in data[p][\"visits\"]:\n",
    "        for d in v[0]:\n",
    "            d = str(d)\n",
    "            if d not in code_to_group:\n",
    "                continue\n",
    "\n",
    "            label[group_to_id[code_to_group[d]]] = 1\n",
    "\n",
    "    data[p][\"labels\"] = label\n",
    "    data[p][\"labels\"] = np.append(data[p][\"labels\"], data[p][\"insurance\"])\n",
    "    data[p][\"labels\"] = np.append(data[p][\"labels\"], data[p][\"ethnicity\"])\n",
    "    data[p][\"labels\"] = np.append(data[p][\"labels\"], data[p][\"gender\"])\n",
    "    data[p][\"labels\"] = np.append(data[p][\"labels\"], data[p][\"isDead\"])\n",
    "\n",
    "# Convert diagnoses, procedures, and medications to text\n",
    "print(\"Converting Codes to Text\")\n",
    "medMapping = {row[\"NDC\"]: row[\"DRUG\"] for _, row in medicationDf.iterrows()}\n",
    "for p in data:\n",
    "    new_visits = []\n",
    "    for v in data[p][\"visits\"]:\n",
    "        new_visit = []\n",
    "        for c in v[0]:\n",
    "            new_visit.append(c)\n",
    "        for c in v[1]:\n",
    "            new_visit.append(c)\n",
    "        for c in v[2]:\n",
    "            if c in medMapping:\n",
    "                new_visit.append(medMapping[c])\n",
    "            else:\n",
    "                new_visit.append(c)\n",
    "\n",
    "        new_visits.append((new_visit, [], [], [v[3]]))\n",
    "\n",
    "        for lab_v in v[4]:\n",
    "            new_visits.append(([], lab_v[0], lab_v[1], lab_v[2]))\n",
    "    data[p][\"visits\"] = new_visits\n",
    "\n",
    "# Convert diagnoses, procedures, and medications to indices\n",
    "\n",
    "print(\"Converting Codes to Indices\")\n",
    "allCodes = list(set([c for p in data for v in data[p][\"visits\"] for c in v[0]]))\n",
    "np.random.shuffle(allCodes)\n",
    "code_to_index = {c: i for i, c in enumerate(allCodes)}\n",
    "counter = 0\n",
    "for p in data:\n",
    "    new_visits = []\n",
    "    for v in data[p][\"visits\"]:\n",
    "        new_visit = []\n",
    "        for c in v[0]:\n",
    "            new_visit.append(code_to_index[c])\n",
    "\n",
    "        new_visits.append((new_visit, v[1], v[2], v[3]))\n",
    "    data[p][\"visits\"] = new_visits\n",
    "\n",
    "index_to_code = {v: k for k, v in code_to_index.items()}\n",
    "data = list(data.values())\n",
    "\n",
    "MAX_TIME_STEPS = 150\n",
    "data = [\n",
    "    {\"labels\": data[i][\"labels\"], \"visits\": data[i][\"visits\"][: MAX_TIME_STEPS - 2]}\n",
    "    for i in range(len(data))\n",
    "]  # 2 for the start and label visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17540961-9e14-4fce-8301-521717280e51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train-Val-Test Split\n",
    "print(\"Splitting Datasets\")\n",
    "train_dataset, test_dataset = train_test_split(\n",
    "    data, test_size=0.4, random_state=4, shuffle=True\n",
    ") # train 60, val 10, test 30\n",
    "train_dataset, val_dataset = train_test_split(\n",
    "    train_dataset, test_size=0.1, random_state=4, shuffle=True\n",
    ")\n",
    "\n",
    "# Save Everything\n",
    "print(\"Saving Everything\")\n",
    "print(len(index_to_code))\n",
    "print(len(data[0][\"labels\"]))\n",
    "pickle.dump(\n",
    "    dict((i, x) for (x, i) in list(group_to_id.items())),\n",
    "    open(\"./data/idToLabel.pkl\", \"wb\"),\n",
    ")\n",
    "pickle.dump(index_to_code, open(\"./data/indexToCode.pkl\", \"wb\"))\n",
    "pickle.dump(data, open(\"./data/allData_1000.pkl\", \"wb\"))\n",
    "pickle.dump(train_dataset, open(\"./data/trainData.pkl\", \"wb\"))\n",
    "pickle.dump(val_dataset, open(\"./data/valData.pkl\", \"wb\"))\n",
    "pickle.dump(test_dataset, open(\"./data/testData.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ceb274-7aee-4949-aebb-1279faaae26b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from pyhealth.metrics.fairness import fairness_metrics_fn\n",
    "from pyhealth.models import RNN, Transformer\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader, split_by_patient\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "\n",
    "allData_5000 = pickle.load(open(\"./data/allData_5000.pkl\",\"rb\"))\n",
    "allData_1000 = allData_5000[0:1000]\n",
    "# Train-Val-Test Split\n",
    "print(\"Splitting Datasets\")\n",
    "train_ehr_data, test_ehr_data = train_test_split(\n",
    "  allData_1000  , test_size=0.1, random_state=4, shuffle=True\n",
    ") # train 80, val 10, test 10\n",
    "train_ehr_data, val_ehr_data = train_test_split(\n",
    "    train_ehr_data, test_size=0.1, random_state=4, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "def transform_data(ehr_dataset):\n",
    "    final_data = []\n",
    "    patient_id = 0  # Starting patient ID\n",
    "\n",
    "    for patient in ehr_dataset:\n",
    "        for i, visit in enumerate(patient[\"visits\"]):\n",
    "            visit_data = {\n",
    "                \"visit_id\": i,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"visit_codes\": [[int(x) for x in visit[0]]],\n",
    "                \"gender\": [[int(float(patient[\"labels\"][26]))]],\n",
    "                \"ethnicity\": [[int(float(patient[\"labels\"][25]))]],\n",
    "                \"disease_label\": [[int(float(x)) for x in patient[\"labels\"][0:25]]],\n",
    "                \"label\": int(float(patient[\"labels\"][27])),\n",
    "            }\n",
    "            final_data.append(visit_data)\n",
    "        patient_id += 1\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def calculate_wtpr(y_true, y_prob, sensitive_attribute, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    subgroups = np.unique(sensitive_attribute)\n",
    "    tpr_scores = {}\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_mask = sensitive_attribute == subgroup\n",
    "        y_true_subgroup = y_true[subgroup_mask]\n",
    "        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "\n",
    "        confusion_mat = confusion_matrix(y_true_subgroup, y_pred_subgroup)\n",
    "\n",
    "        if confusion_mat.size == 1:\n",
    "            if y_true_subgroup[0] == 1:\n",
    "                tp = confusion_mat[0, 0]\n",
    "                fn = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fn = confusion_mat[0, 0]\n",
    "            tn = fp = 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        tpr_scores[subgroup] = tpr\n",
    "\n",
    "    wtpr = np.mean(list(tpr_scores.values()))\n",
    "    return wtpr\n",
    "\n",
    "transformed_train_ehr_dataset = transform_data(train_ehr_data)\n",
    "transformed_val_ehr_dataset = transform_data(val_ehr_data)\n",
    "transformed_test_ehr_dataset = transform_data(test_ehr_data)\n",
    "transformed_allData_1000 = transform_data(allData_1000)\n",
    "\n",
    "max_visit_codes_length = max(\n",
    "    len(sample[\"visit_codes\"][0]) for sample in transformed_allData_1000\n",
    ")\n",
    "for sample in transformed_train_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_val_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_test_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "\n",
    "formatted_train_ehr_dataset = SampleEHRDataset(samples=transformed_train_ehr_dataset)\n",
    "formatted_val_ehr_dataset = SampleEHRDataset(samples=transformed_val_ehr_dataset)\n",
    "formatted_test_ehr_dataset = SampleEHRDataset(samples=transformed_test_ehr_dataset)\n",
    "\n",
    "k = 5  # Number of folds\n",
    "fairness_scores = {\n",
    "    \"disparate_impact\": [],\n",
    "    \"statistical_parity_difference\": [],\n",
    "    \"wtpr\": [],\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "formatted_combined_ehr_dataset = formatted_train_ehr_dataset + formatted_val_ehr_dataset\n",
    "\n",
    "for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "    fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "    fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "\n",
    "    transformermodel = Transformer(\n",
    "        dataset=formatted_train_ehr_dataset,\n",
    "        # look up what are available for \"feature_keys\" and \"label_keys\" in dataset.samples[0]\n",
    "        feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "        label_key=\"label\",\n",
    "        mode=\"binary\",\n",
    "    )\n",
    "\n",
    "    train_loader = get_dataloader(fold_train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = get_dataloader(fold_val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    trainer = Trainer(model=transformermodel)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=30,\n",
    "        monitor=\"pr_auc\",\n",
    "    )\n",
    "\n",
    "    y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "\n",
    "    # protected_group = 1  # female in gender\n",
    "    # # Prepare the sensitive attribute array for the validation set\n",
    "    # sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    # for idx, visit in enumerate(fold_val_dataset):\n",
    "    #     sensitive_attribute_value = visit[\"gender\"][0][0]\n",
    "    #     if sensitive_attribute_value == protected_group:\n",
    "    #         sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    # Prepare the sensitive attribute array for the validation set\n",
    " \n",
    "    unprotected_group = 0  # white in eth\n",
    "    sensitive_attribute_array= np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    for idx,visit in enumerate(fold_val_dataset):\n",
    "            sensitive_attribute_value = visit[\"ethnicity\"][0][0]\n",
    "            if sensitive_attribute_value != unprotected_group:\n",
    "                sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    \n",
    "    # Calculate fairness metrics for the current fold\n",
    "    fold_fairness_metrics = fairness_metrics_fn(\n",
    "        y_true,\n",
    "        y_prob,\n",
    "        sensitive_attributes=sensitive_attribute_array,\n",
    "        favorable_outcome=1,\n",
    "        metrics=None,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "\n",
    "    # Append the fairness metrics for the current fold\n",
    "    fairness_scores[\"disparate_impact\"].append(\n",
    "        fold_fairness_metrics[\"disparate_impact\"]\n",
    "    )\n",
    "    fairness_scores[\"statistical_parity_difference\"].append(\n",
    "        fold_fairness_metrics[\"statistical_parity_difference\"]\n",
    "    )\n",
    "    fairness_scores[\"wtpr\"].append(wtpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c16024-7aaa-45aa-8b5c-e7f6e02478b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the mean and standard deviation for each fairness metric\n",
    "fairness_metrics = {}\n",
    "for metric, scores in fairness_scores.items():\n",
    "    values = scores\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    print(f\"{metric}: Mean = {mean:.4f}, Std = {std:.4f}\")\n",
    "    fairness_metrics[metric] = mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21147003-469a-48ca-92a3-9516d410b7a9",
   "metadata": {},
   "source": [
    "# Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e5326-68c4-4707-ad6b-0643cc8d5557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "trainData = pickle.load(open(\"data/trainData.pkl\", \"rb\"))\n",
    "valData = pickle.load(open(\"data/valData.pkl\", \"rb\"))\n",
    "idToLab = pickle.load(open(\"./data/idx_to_lab.pkl\", \"rb\"))\n",
    "labToNumber = {\n",
    "    l: i for (i, l) in enumerate(pickle.load(open(\"./data/id_to_channel.pkl\", \"rb\")))\n",
    "}\n",
    "isCategorical = pickle.load(open(\"./data/is_categorical_channel.pkl\", \"rb\"))\n",
    "beginPos = pickle.load(open(\"./data/begin_pos.pkl\", \"rb\"))\n",
    "possibleValues = pickle.load(open(\"./data/possible_values.pkl\", \"rb\"))\n",
    "variableRanges = pickle.load(open(\"./data/variable_ranges.pkl\", \"rb\"))\n",
    "discretization = {\n",
    "    \"Diastolic blood pressure\": [\n",
    "        0,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        65,\n",
    "        70,\n",
    "        75,\n",
    "        80,\n",
    "        85,\n",
    "        90,\n",
    "        95,\n",
    "        100,\n",
    "        105,\n",
    "        110,\n",
    "        120,\n",
    "        130,\n",
    "        375,\n",
    "    ],\n",
    "    \"Fraction inspired oxygen\": [\n",
    "        0.2,\n",
    "        0.3,\n",
    "        0.4,\n",
    "        0.5,\n",
    "        0.6,\n",
    "        0.7,\n",
    "        0.8,\n",
    "        0.9,\n",
    "        1.0,\n",
    "        1.001,\n",
    "        1.1,\n",
    "    ],\n",
    "    \"Glucose\": [\n",
    "        0,\n",
    "        40,\n",
    "        60,\n",
    "        80,\n",
    "        100,\n",
    "        110,\n",
    "        120,\n",
    "        130,\n",
    "        140,\n",
    "        150,\n",
    "        160,\n",
    "        170,\n",
    "        180,\n",
    "        200,\n",
    "        225,\n",
    "        275,\n",
    "        325,\n",
    "        400,\n",
    "        600,\n",
    "        800,\n",
    "        1000,\n",
    "        2200,\n",
    "    ],\n",
    "    \"Heart Rate\": [0, 40, 50, 60, 70, 80, 90, 100, 110, 120, 140, 160, 180, 200, 390],\n",
    "    \"Height\": [0, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 230],\n",
    "    \"Mean blood pressure\": [\n",
    "        0,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "        80,\n",
    "        90,\n",
    "        100,\n",
    "        110,\n",
    "        120,\n",
    "        130,\n",
    "        140,\n",
    "        150,\n",
    "        160,\n",
    "        180,\n",
    "        200,\n",
    "        375,\n",
    "    ],\n",
    "    \"Oxygen saturation\": [\n",
    "        0,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        55,\n",
    "        60,\n",
    "        65,\n",
    "        70,\n",
    "        75,\n",
    "        80,\n",
    "        85,\n",
    "        90,\n",
    "        100,\n",
    "        100.001,\n",
    "        150,\n",
    "    ],\n",
    "    \"pH\": [6.3, 6.7, 7.1, 7.35, 7.45, 7.6, 8.0, 8.3, 10],\n",
    "    \"Respiratory rate\": [0, 6, 8, 10, 12, 14, 16, 18, 20, 25, 30, 35, 330],\n",
    "    \"Systolic blood pressure\": [\n",
    "        0,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "        80,\n",
    "        90,\n",
    "        100,\n",
    "        110,\n",
    "        120,\n",
    "        130,\n",
    "        140,\n",
    "        150,\n",
    "        160,\n",
    "        170,\n",
    "        180,\n",
    "        190,\n",
    "        200,\n",
    "        210,\n",
    "        230,\n",
    "        375,\n",
    "    ],\n",
    "    \"Temperature\": [\n",
    "        14.2,\n",
    "        30,\n",
    "        32,\n",
    "        33,\n",
    "        33.5,\n",
    "        34,\n",
    "        34.5,\n",
    "        35,\n",
    "        35.5,\n",
    "        36,\n",
    "        36.5,\n",
    "        37,\n",
    "        37.5,\n",
    "        38,\n",
    "        38.5,\n",
    "        39,\n",
    "        39.5,\n",
    "        40,\n",
    "        47,\n",
    "    ],\n",
    "    \"Weight\": [\n",
    "        0,\n",
    "        30,\n",
    "        40,\n",
    "        45,\n",
    "        50,\n",
    "        55,\n",
    "        60,\n",
    "        65,\n",
    "        70,\n",
    "        75,\n",
    "        80,\n",
    "        85,\n",
    "        90,\n",
    "        95,\n",
    "        100,\n",
    "        105,\n",
    "        110,\n",
    "        115,\n",
    "        120,\n",
    "        125,\n",
    "        130,\n",
    "        135,\n",
    "        140,\n",
    "        145,\n",
    "        150,\n",
    "        160,\n",
    "        170,\n",
    "        190,\n",
    "        210,\n",
    "        250,\n",
    "    ],\n",
    "    \"Age\": [18, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90],\n",
    "    \"Days\": [0, 11, 16, 21, 25, 30.1, 35.1, 43, 48, 54, 60, 66, 72, 81, 90, 100.1],\n",
    "    \"Hours\": [\n",
    "        0,\n",
    "        0.5,\n",
    "        1.5,\n",
    "        2.5,\n",
    "        3.5,\n",
    "        6.5,\n",
    "        10.5,\n",
    "        16.5,\n",
    "        26.5,\n",
    "        48.0,\n",
    "        48.1,\n",
    "        60.1,\n",
    "        80.1,\n",
    "        110.1,\n",
    "        150.1,\n",
    "        200.1,\n",
    "    ],\n",
    "}\n",
    "\n",
    "formatMap = {\n",
    "    \"Diastolic blood pressure\": (\".0f\", int),\n",
    "    \"Fraction inspired oxygen\": (\".2f\", float),\n",
    "    \"Glucose\": (\".0f\", int),\n",
    "    \"Heart Rate\": (\".0f\", int),\n",
    "    \"Height\": (\".0f\", int),\n",
    "    \"Mean blood pressure\": (\".0f\", int),\n",
    "    \"Oxygen saturation\": (\".0f\", int),\n",
    "    \"pH\": (\".2f\", float),\n",
    "    \"Respiratory rate\": (\".0f\", int),\n",
    "    \"Systolic blood pressure\": (\".0f\", int),\n",
    "    \"Temperature\": (\".1f\", float),\n",
    "    \"Weight\": (\".1f\", float),\n",
    "    \"Age\": (\".2f\", float),\n",
    "    \"Days\": (\".2f\", float),\n",
    "    \"Hours\": (\".1f\", float),\n",
    "}\n",
    "\n",
    "\n",
    "def get_index(mapping, key, value):\n",
    "    possible_values = mapping[key]\n",
    "    for i in range(len(possible_values) - 1):\n",
    "        if value < possible_values[i + 1]:\n",
    "            return i\n",
    "\n",
    "    print(f\"{value} for {key} not in {possible_values}\")\n",
    "    return len(possible_values) - 2\n",
    "\n",
    "\n",
    "# Convert to New Data Format\n",
    "for p in trainData + valData:\n",
    "    new_visits = []\n",
    "    firstVisit = True\n",
    "    for v in p[\"visits\"]:\n",
    "        if v[1] == []:\n",
    "            new_cont = get_index(\n",
    "                discretization, \"Age\" if firstVisit else \"Days\", v[3][-1]\n",
    "            )\n",
    "            firstVisit = False\n",
    "            new_visits.append((v[0], [], [], [new_cont]))\n",
    "        else:\n",
    "            new_labs = []\n",
    "            new_values = []\n",
    "            for l, val in zip(v[1], v[2]):\n",
    "                if isCategorical[idToLab[l]]:\n",
    "                    if val == 1:\n",
    "                        new_labs.append(labToNumber[idToLab[l]])\n",
    "                        new_values.append(beginPos[labToNumber[idToLab[l]]] - l)\n",
    "                else:\n",
    "                    if (\n",
    "                        val < variableRanges[idToLab[l]][0]\n",
    "                        or val >= variableRanges[idToLab[l]][1]\n",
    "                    ):\n",
    "                        continue\n",
    "\n",
    "                    new_labs.append(labToNumber[idToLab[l]])\n",
    "                    new_values.append(get_index(discretization, idToLab[l], val))\n",
    "\n",
    "            if not new_labs:\n",
    "                continue\n",
    "            new_cont = get_index(discretization, \"Hours\", v[3][-1])\n",
    "            new_visits.append((v[0], new_labs, new_values, [new_cont]))\n",
    "\n",
    "    p[\"visits\"] = new_visits\n",
    "\n",
    "pickle.dump(trainData, open(\"./discretized_data/trainDataset.pkl\", \"wb\"))\n",
    "pickle.dump(valData, open(\"./discretized_data/valDataset.pkl\", \"wb\"))\n",
    "\n",
    "newIdToLab = {i: l for (l, i) in labToNumber.items()}\n",
    "newBeginPos = []\n",
    "seenContinuous = False\n",
    "for i in range(len(newIdToLab)):\n",
    "    if not seenContinuous:\n",
    "        newBeginPos.append(beginPos[i])\n",
    "        if not isCategorical[newIdToLab[i]]:\n",
    "            seenContinuous = True\n",
    "            currPos = newBeginPos[i] + len(discretization[newIdToLab[i]]) - 1\n",
    "    else:\n",
    "        newBeginPos.append(currPos)\n",
    "        currPos += len(discretization[newIdToLab[i]]) - 1\n",
    "\n",
    "newIdxToId = {}\n",
    "for i in range(len(newBeginPos) - 1):\n",
    "    for j in range(newBeginPos[i], newBeginPos[i + 1]):\n",
    "        newIdxToId[j] = i\n",
    "for j in range(\n",
    "    newBeginPos[-1],\n",
    "    newBeginPos[-1] + len(discretization[newIdToLab[len(newBeginPos) - 1]]) - 1,\n",
    "):\n",
    "    newIdxToId[j] = len(newBeginPos) - 1\n",
    "\n",
    "pickle.dump(newIdxToId, open(\"discretized_data/idxToId.pkl\", \"wb\"))\n",
    "pickle.dump(formatMap, open(\"discretized_data/formatMap.pkl\", \"wb\"))\n",
    "pickle.dump(newIdToLab, open(\"discretized_data/idToLab.pkl\", \"wb\"))\n",
    "pickle.dump(newBeginPos, open(\"discretized_data/beginPos.pkl\", \"wb\"))\n",
    "pickle.dump(isCategorical, open(\"discretized_data/isCategorical.pkl\", \"wb\"))\n",
    "pickle.dump(possibleValues, open(\"discretized_data/possibleValues.pkl\", \"wb\"))\n",
    "pickle.dump(discretization, open(\"discretized_data/discretization.pkl\", \"wb\"))\n",
    "\n",
    "print(f\"NUM LABS: {newBeginPos[-1] + len(discretization[newIdToLab[16]]) - 1}\")\n",
    "print(f\"NUM CONTINUOUS: {len(discretization['Age']) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646103-ac07-4814-b38e-7177163d488d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0c21c-abd7-4655-8192-224cc3fe35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    code by Brandon Theodorou\n",
    "    Original GPT-2 Paper and repository here: https://github.com/openai/gpt-2\n",
    "    Original GPT-2 Pytorch Model: https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    GPT-2 Pytorch Model Derived From: https://github.com/graykode/gpt-2-Pytorch\n",
    "'''\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# GELU Activation and Layer Normalization:\n",
    "# gelu(x): Gaussian Error Linear Unit (GELU) activation function.\n",
    "# LayerNorm: Layer normalization module with learnable parameters.\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias\n",
    "    \n",
    "# 1D convolutional layer with learnable weight and bias parameters.\n",
    "class Conv1D(nn.Module):\n",
    "    def __init__(self, nf, nx):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.nf = nf\n",
    "        w = torch.empty(nx, nf)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w)\n",
    "        self.bias = nn.Parameter(torch.zeros(nf))\n",
    "\n",
    "    def forward(self, x):\n",
    "        size_out = x.size()[:-1] + (self.nf,)\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        x = x.view(*size_out)\n",
    "        return x\n",
    "    \n",
    "# Self-attention mechanism with scaled dot-product attention. It includes convolutional layers for query, key, and value projections.\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state * 3, nx)\n",
    "        self.c_proj = Conv1D(n_state, nx)\n",
    "\n",
    "    def _attn(self, q, k, v):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        b = self.bias[:, :, ns-nd:ns, :ns]\n",
    "        w = w * b - 1e10 * (1 - b)\n",
    "        w = nn.Softmax(dim=-1)(w)\n",
    "        return torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
    "            key = torch.cat((past_key, key), dim=-1)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
    "        a = self._attn(query, key, value)\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        return a, present\n",
    "    \n",
    "# Multi-Layer Perceptron module with a fully connected layer, activation function (GELU), and another fully connected layer.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n",
    "        super(MLP, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.c_fc = Conv1D(n_state, nx)\n",
    "        self.c_proj = Conv1D(nx, n_state)\n",
    "        self.act = gelu\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.act(self.c_fc(x))\n",
    "        h2 = self.c_proj(h)\n",
    "        return h2\n",
    "\n",
    "# A block containing layer normalization, attention mechanism, and an MLP. These blocks are stacked to form the transformer model.\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_ctx, config, scale=False):\n",
    "        super(Block, self).__init__()\n",
    "        nx = config.n_embd\n",
    "        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.attn = Attention(nx, n_ctx, config, scale)\n",
    "        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(4 * nx, config)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        a, present = self.attn(self.ln_1(x), layer_past=layer_past)\n",
    "        x = x + a\n",
    "        m = self.mlp(self.ln_2(x))\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "# The main transformer model composed of stacked blocks. It includes positional and visit embeddings.\n",
    "class CoarseTransformerModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CoarseTransformerModel, self).__init__()\n",
    "        self.n_layer = config.n_layer\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_vocab = config.total_vocab_size\n",
    "\n",
    "        self.vis_embed_mat = nn.Linear(config.total_vocab_size, config.n_embd, bias=False)\n",
    "        self.pos_embed_mat = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        block = Block(config.n_ctx, config, scale=True)\n",
    "        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "\n",
    "    def forward(self, input_visits, position_ids=None, past=None):\n",
    "        if past is None:\n",
    "            past_length = 0\n",
    "            past = [None] * len(self.h)\n",
    "        else:\n",
    "            past_length = past[0][0].size(-2)\n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(past_length, input_visits.size(1) + past_length, dtype=torch.long,\n",
    "                                        device=input_visits.device)\n",
    "            position_ids = position_ids.unsqueeze(0).expand(input_visits.size(0), input_visits.size(1))\n",
    "\n",
    "        inputs_embeds = self.vis_embed_mat(input_visits)\n",
    "        position_embeds = self.pos_embed_mat(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        for block, layer_past in zip(self.h, past):\n",
    "            hidden_states, _ = block(hidden_states, layer_past)\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# Linear layer with a configurable mask on the weights, ensuring an autoregressive property.    \n",
    "class AutoregressiveLinear(nn.Linear):\n",
    "    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__(in_features, out_features, bias)        \n",
    "        self.register_buffer('mask', torch.tril(torch.ones(in_features, out_features)).int())\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.mask * self.weight, self.bias)\n",
    "\n",
    "    #A specific head that uses autoregressive linear layers for generating synthetic EHR data.\n",
    "class FineAutoregressiveHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(FineAutoregressiveHead, self).__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "        self.total_vocab_size = config.total_vocab_size\n",
    "\n",
    "        self.auto1 = AutoregressiveLinear(config.n_embd + self.total_vocab_size, config.n_embd + self.total_vocab_size)\n",
    "        self.auto2 = AutoregressiveLinear(config.n_embd + self.total_vocab_size, config.n_embd + self.total_vocab_size)\n",
    "\n",
    "    def forward(self, history, input_visits):\n",
    "        history = history[:,:-1,:]\n",
    "        input_visits = input_visits[:,1:,:]\n",
    "        code_logits = self.auto2(torch.relu(self.auto1(torch.cat((history, input_visits), dim=2))))[:,:,self.n_embd-1:-1]\n",
    "        return code_logits\n",
    "\n",
    "    def sample(self, history, input_visits):\n",
    "        history = history[:,:-1,:]\n",
    "        input_visits = input_visits[:,1:,:]\n",
    "        currVisit = torch.cat((history, input_visits), dim=2)[:,-1,:].unsqueeze(1)\n",
    "        code_logits = self.auto2(torch.relu(self.auto1(currVisit)))[:,:,self.n_embd-1:-1]\n",
    "        return code_logits\n",
    "\n",
    "class HALOModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(HALOModel, self).__init__()\n",
    "        self.transformer = CoarseTransformerModel(config) # visit level\n",
    "        self.ehr_head = FineAutoregressiveHead(config) # code level\n",
    "        self.total_vocab_size = config.total_vocab_size\n",
    "\n",
    "    def disparate_impact_loss_gen(self, code_probs, input_genders):\n",
    "        # Get the unique gender labels\n",
    "        gender_labels = torch.unique(input_genders)\n",
    "        \n",
    "        # Initialize variables to store the sum and count of positive predictions for each gender group\n",
    "        gender_pos_sum = torch.zeros(len(gender_labels), device=code_probs.device)\n",
    "        gender_pos_count = torch.zeros(len(gender_labels), device=code_probs.device)\n",
    "        \n",
    "        # Iterate over each patient\n",
    "        for i in range(code_probs.size(0)):\n",
    "            gender = input_genders[i]\n",
    "            gender_idx = (gender_labels == gender).nonzero(as_tuple=True)[0]\n",
    "            \n",
    "            # Count the number of positive predictions for each gender group\n",
    "            pos_pred = (code_probs[i] > 0.5).sum().item()\n",
    "            gender_pos_sum[gender_idx] += pos_pred\n",
    "            gender_pos_count[gender_idx] += 1\n",
    "        \n",
    "        # Calculate the positive prediction rate for each gender group\n",
    "        gender_pos_rate = gender_pos_sum / gender_pos_count\n",
    "\n",
    "        # Compute the disparate impact ratio\n",
    "        if len(gender_pos_rate) > 1:\n",
    "            di_ratio = gender_pos_rate[0] / gender_pos_rate[1]\n",
    "        else:\n",
    "            di_ratio = torch.tensor(1.0)  # Set di_ratio to 1 if there is only one gender group\n",
    "        \n",
    "        # Calculate the disparate impact loss\n",
    "        di_loss = torch.abs(1 - di_ratio)\n",
    "        \n",
    "        return di_loss\n",
    "\n",
    "    def disparate_impact_loss_eth(self, code_probs, input_ethnicities):\n",
    "        # Get the unique ethnicity labels\n",
    "        #print(code_probs)\n",
    "        ethnicity_labels = torch.unique(input_ethnicities)\n",
    "    \n",
    "        # Initialize variables to store the sum and count of positive predictions for each ethnicity group\n",
    "        ethnicity_pos_sum = torch.zeros(len(ethnicity_labels), device=code_probs.device)\n",
    "        ethnicity_pos_count = torch.zeros(len(ethnicity_labels), device=code_probs.device)\n",
    "    \n",
    "        # Iterate over each patient\n",
    "        for i in range(code_probs.size(0)):\n",
    "            ethnicity = input_ethnicities[i]\n",
    "            ethnicity_idx = (ethnicity_labels == ethnicity).nonzero(as_tuple=True)[0]\n",
    "    \n",
    "            # Count the number of positive predictions for each ethnicity group\n",
    "            pos_pred = (code_probs[i] > 0.5).sum().item()\n",
    "            ethnicity_pos_sum[ethnicity_idx] += pos_pred\n",
    "            ethnicity_pos_count[ethnicity_idx] += 1\n",
    "    \n",
    "        # Calculate the positive prediction rate for each ethnicity group\n",
    "        ethnicity_pos_rate = ethnicity_pos_sum / ethnicity_pos_count\n",
    "    \n",
    "        # Compute the disparate impact ratio\n",
    "        if len(ethnicity_pos_rate) > 1:\n",
    "            di_ratio = ethnicity_pos_rate[0] / ethnicity_pos_rate[1]\n",
    "        else:\n",
    "            di_ratio = torch.tensor(1.0)  # Set di_ratio to 1 if there is only one ethnicity group\n",
    "    \n",
    "        # Calculate the disparate impact loss\n",
    "        di_loss = torch.abs(1 - di_ratio)\n",
    "    \n",
    "        return di_loss\n",
    "        \n",
    "    def forward(self, input_visits, input_eth, fairness_metrics, input_ethnicities=None, position_ids=None, ehr_labels=None, ehr_masks=None, past=None, pos_loss_weight=None):\n",
    "        \n",
    "        hidden_states = self.transformer(input_visits, position_ids, past)\n",
    "        code_logits = self.ehr_head(hidden_states, input_visits)\n",
    "        sig = nn.Sigmoid()\n",
    "        code_probs = sig(code_logits)\n",
    "        if ehr_labels is not None:    \n",
    "            shift_labels = torch.clamp(ehr_labels[..., 1:, :].contiguous(), min=0.0, max=1.0)\n",
    "            loss_weights = None\n",
    "            if pos_loss_weight is not None:\n",
    "                loss_weights = torch.ones(code_probs.shape, device=code_probs.device)\n",
    "                loss_weights = loss_weights + (pos_loss_weight-1) * shift_labels\n",
    "            if ehr_masks is not None:\n",
    "                code_probs = code_probs * ehr_masks\n",
    "                shift_labels = shift_labels * ehr_masks\n",
    "                if pos_loss_weight is not None:\n",
    "                    loss_weights = loss_weights * ehr_masks\n",
    "\n",
    "            bce = nn.BCELoss(weight=loss_weights)\n",
    "            loss = bce(code_probs, shift_labels)\n",
    "            if input_eth is not None:\n",
    "                di_loss = self.disparate_impact_loss_eth(code_probs, input_eth)\n",
    "                if fairness_metrics is None:\n",
    "                    loss = loss + 1 * di_loss\n",
    "                else:\n",
    "                    disparate_impact = torch.tensor(fairness_metrics['disparate_impact'], dtype=loss.dtype, device='cuda:0')\n",
    "                    loss = (loss + disparate_impact).mean()\n",
    "            return loss, code_probs, shift_labels\n",
    "        return code_probs\n",
    "            \n",
    "\n",
    "    def sample(self, input_visits, random=True):\n",
    "        sig = nn.Sigmoid()\n",
    "        hidden_states = self.transformer(input_visits)\n",
    "        i = 0\n",
    "        while i < self.total_vocab_size:\n",
    "            next_logits = self.ehr_head.sample(hidden_states, input_visits)\n",
    "            next_probs = sig(next_logits)\n",
    "            if random:\n",
    "                visit = torch.bernoulli(next_probs)\n",
    "            else:\n",
    "                visit = torch.round(next_probs)\n",
    "\n",
    "            remaining_visit = visit[:,0,i:]\n",
    "            nonzero = torch.nonzero(remaining_visit, as_tuple=True)[1]\n",
    "            if nonzero.numel() == 0:\n",
    "                break\n",
    "\n",
    "            first_nonzero = nonzero.min()\n",
    "            input_visits[:,-1,i + first_nonzero] = visit[:,0,i + first_nonzero]\n",
    "            i = i + first_nonzero + 1\n",
    "            \n",
    "        return input_visits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a7db7e-0396-4965-b2fe-1b2646df0cc4",
   "metadata": {},
   "source": [
    "# Model Configuration\n",
    "\n",
    "Depending on any dataset changes, you may need to adjust the config function file below to the dataset you are using. Specifically, you may need to set `code_vocab_size` and `label_vocab_size` based on what is printed at the end of running the data generation part and then set `lab_vocab_size` and `continuous_vocab_size` based on what is printed at the end of running the 'discretization'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd6ab07-cde6-4e14-bc72-0c063a67e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "\n",
    "class HALOConfig(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            total_vocab_size=4101,\n",
    "            code_vocab_size=3817,\n",
    "            lab_vocab_size=237,\n",
    "            continuous_vocab_size=15,\n",
    "            label_vocab_size=29,\n",
    "            special_vocab_size=3,\n",
    "\n",
    "            categorical_lab_vocab_size=47,\n",
    "            continuous_lab_vocab_size=190,\n",
    "            \n",
    "            phenotype_labels=25, \n",
    "            ethnicity_labels=10, \n",
    "            gender_labels=2,\n",
    "\n",
    "            hidden_size = 128,\n",
    "        \n",
    "            fairness_weight = 1.0,\n",
    "            \n",
    "            n_positions=150,\n",
    "            n_ctx=150, #context size\n",
    "            n_embd=1440,\n",
    "            n_layer=12,\n",
    "            n_head=18,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            initializer_range=0.02,\n",
    "            \n",
    "            batch_size=10,\n",
    "            sample_batch_size=25,\n",
    "            epoch=1,\n",
    "            lr=1e-4,\n",
    "    ):\n",
    "        self.total_vocab_size = total_vocab_size\n",
    "        self.code_vocab_size = code_vocab_size\n",
    "        self.label_vocab_size = label_vocab_size\n",
    "        self.lab_vocab_size = lab_vocab_size\n",
    "        self.categorical_lab_vocab_size = categorical_lab_vocab_size\n",
    "        self.continuous_lab_vocab_size = continuous_lab_vocab_size\n",
    "        self.continuous_vocab_size = continuous_vocab_size\n",
    "        self.special_vocab_size = special_vocab_size\n",
    "        self.phenotype_labels = phenotype_labels\n",
    "        self.fairness_weight = fairness_weight\n",
    "        self.gender_labels = gender_labels\n",
    "        self.ethnicity_labels = ethnicity_labels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_positions = n_positions\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.initializer_range = initializer_range\n",
    "        self.batch_size = batch_size\n",
    "        self.sample_batch_size = sample_batch_size\n",
    "        self.epoch = epoch\n",
    "        self.lr = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b6105f-f777-49c5-b7b9-a6cace72f381",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "The following cell can be run to train the model. Before running, please create an empty `save/` directory and run the cells below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4d28d-17ce-4dba-ac7e-8f69fe8c4999",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 4\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "config = HALOConfig()\n",
    "\n",
    "local_rank = -1\n",
    "fp16 = False\n",
    "if local_rank == -1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(device)\n",
    "train_ehr_dataset = pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\"))\n",
    "val_ehr_dataset = pickle.load(open(\"discretized_data/valDataset.pkl\", \"rb\"))\n",
    "\n",
    "# Convert to fully codes\n",
    "beginPos = pickle.load(open(\"discretized_data/beginPos.pkl\", \"rb\"))\n",
    "for p in train_ehr_dataset + val_ehr_dataset:\n",
    "    new_visits = []\n",
    "    for v in p[\"visits\"]:\n",
    "        new_idx = v[0]\n",
    "        for l, val in zip(v[1], v[2]):\n",
    "            new_idx.append(config.code_vocab_size + beginPos[l] + val)\n",
    "        new_idx.append(config.code_vocab_size + config.lab_vocab_size + v[3][-1])\n",
    "        new_visits.append(new_idx)\n",
    "\n",
    "    p[\"visits\"] = new_visits\n",
    "\n",
    "def get_batch(loc, batch_size, mode):\n",
    "    if mode == \"train\":\n",
    "        ehr = train_ehr_dataset[loc : loc + batch_size]\n",
    "    elif mode == \"valid\":\n",
    "        ehr = val_ehr_dataset[loc : loc + batch_size]\n",
    "    else:\n",
    "        ehr = test_ehr_dataset[loc : loc + batch_size]\n",
    "\n",
    "    batch_gender = np.zeros((len(ehr)))\n",
    "    batch_eth = np.zeros((len(ehr)))\n",
    "    batch_ehr = np.zeros(\n",
    "        (len(ehr), config.n_ctx, config.total_vocab_size)\n",
    "    )  # 3d array len(ehr) * config.n_ctx * config.total_vocab_size\n",
    "    batch_mask = np.zeros(\n",
    "        (len(ehr), config.n_ctx, 1)\n",
    "    )  # 3d array len(ehr) * config.n_ctx * 1\n",
    "    for i, p in enumerate(ehr):\n",
    "        visits = p['visits']\n",
    "        #print(f\"Lenght of visits{len(visits)}\")\n",
    "        \n",
    "        for i, p in enumerate(ehr):\n",
    "            visits = p['visits']\n",
    "            for j, v in enumerate(visits):\n",
    "                try:\n",
    "                    batch_ehr[i, j+2][v] = 1\n",
    "                except IndexError:\n",
    "                # Handle the out-of-bounds index\n",
    "                    print(f\"Warning: Index {v} is out of bounds for batch_ehr[{i}, {j+2}]\")\n",
    "                    continue\n",
    "                batch_mask[i, j+2] = 1\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size : config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size,\n",
    "        ] = np.array(\n",
    "            p[\"labels\"]\n",
    "        )  # Set the patient labels\n",
    "\n",
    "        batch_eth[i] = batch_ehr[\n",
    "            i,\n",
    "            1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            - 3,\n",
    "        ]\n",
    "\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            len(visits) + 1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            + 1,\n",
    "        ] = 1  # Set the final visit to have the end token\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            len(visits) + 2 :,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            + 2,\n",
    "        ] = 1  # Set the rest to the padded visit token\n",
    "\n",
    "    batch_mask[:, 1] = 1  # Set the mask to cover the labels\n",
    "    batch_ehr[\n",
    "        :,\n",
    "        0,\n",
    "        config.code_vocab_size\n",
    "        + config.lab_vocab_size\n",
    "        + config.continuous_vocab_size\n",
    "        + config.label_vocab_size,\n",
    "    ] = 1  # Set the first visits to be the start token\n",
    "    batch_mask = batch_mask[\n",
    "        :, 1:, :\n",
    "    ]  # Shift the mask to match the shifted labels and predictions the model will return\n",
    "\n",
    "    return batch_ehr, batch_mask, batch_eth\n",
    "\n",
    "\n",
    "def shuffle_training_data(train_ehr_dataset):\n",
    "    np.random.shuffle(train_ehr_dataset)\n",
    "\n",
    "model = HALOModel(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# if os.path.exists(\"./save/halo_model\"):\n",
    "#     print(\"Loading previous model\")\n",
    "#     checkpoint = torch.load(\"./save/halo_model\", map_location=torch.device(device))\n",
    "#     model.load_state_dict(checkpoint[\"model\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "# Train Model\n",
    "\n",
    "global_loss = 1e10\n",
    "iteration = 0\n",
    "for e in tqdm(range(config.epoch)):\n",
    "    shuffle_training_data(train_ehr_dataset)\n",
    "    for i in range(0, len(train_ehr_dataset), config.batch_size):\n",
    "        model.train()\n",
    "\n",
    "        batch_ehr, batch_mask, batch_eth = get_batch(i, config.batch_size, \"train\")\n",
    "        batch_eth = torch.tensor(batch_eth, dtype=torch.float32).to(device)\n",
    "        batch_ehr = torch.tensor(batch_ehr, dtype=torch.float32).to(device)\n",
    "        batch_mask = torch.tensor(batch_mask, dtype=torch.float32).to(device)\n",
    "        #print(batch_mask.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss, _, _ = model(\n",
    "            batch_ehr,\n",
    "            batch_eth,\n",
    "            fairness_metrics = None,\n",
    "            input_ethnicities=None,\n",
    "            position_ids=None,\n",
    "            ehr_labels=batch_ehr,\n",
    "            ehr_masks=batch_mask,\n",
    "        )\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if i % (50 * config.batch_size) == 0:\n",
    "        #     print(\"Epoch %d, Iter %d: Training Loss:%.6f\" % (e, i, loss))\n",
    "   \n",
    "        # if i % (250 * len(train_ehr_dataset)) == 0:  # Condition based on iterations\n",
    "        #     if i == 0:\n",
    "        #         continue\n",
    "\n",
    "        if i % (25*config.batch_size) == 0:\n",
    "            print(\"Epoch %d, Iter %d: Training Loss:%.6f\"%(e, i, loss))\n",
    "        if i % (100*config.batch_size) == 0:\n",
    "            if i == 0:\n",
    "               continue \n",
    "            print(\"I am entering eval stage\")\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_l = []\n",
    "                for v_i in range(0, len(val_ehr_dataset), config.batch_size):\n",
    "                    batch_ehr, batch_mask, batch_eth= get_batch(\n",
    "                        v_i, config.batch_size, \"valid\"\n",
    "                    )\n",
    "                    batch_eth = torch.tensor(batch_eth, dtype=torch.float32).to(\n",
    "                        device\n",
    "                    )\n",
    "                    # batch_ethnicities = torch.tensor(batch_ethnicities,dtype=torch.float32).to(device)\n",
    "                    batch_ehr = torch.tensor(batch_ehr, dtype=torch.float32).to(device)\n",
    "                    batch_mask = torch.tensor(batch_mask, dtype=torch.float32).to(\n",
    "                        device\n",
    "                    )\n",
    "\n",
    "                    val_loss, _, _ = model(\n",
    "                        batch_ehr,\n",
    "                        batch_eth,\n",
    "                        fairness_metrics = None,\n",
    "                        input_ethnicities=None,\n",
    "                        position_ids=None,\n",
    "                        ehr_labels=batch_ehr,\n",
    "                        ehr_masks=batch_mask,\n",
    "                    )\n",
    "                    val_l.append((val_loss).cpu().detach().numpy())\n",
    "\n",
    "                cur_val_loss = np.mean(val_l)\n",
    "                print(\"Epoch %d Validation Loss:%.7f\" % (e, cur_val_loss))\n",
    "                if cur_val_loss < global_loss:\n",
    "                    global_loss = cur_val_loss\n",
    "                    state = {\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"iteration\": i,\n",
    "                    }\n",
    "                    torch.save(state, \"./save/generator_model\")\n",
    "                    print(\"\\n------------ Save best model ------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f93f24-b0e5-40b3-a113-d068e7b90ebd",
   "metadata": {},
   "source": [
    "# Generate\n",
    "Ensure that the path `results/datasets` is created before running the following cell and the 'Convert discrete data to cont.' cell.\n",
    "\n",
    "Note, if you want a different amount of data rather than the size of the training dataset, set the totEHRs variable on line 138 of the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91144a8a-4301-41de-b4b0-3311762727cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from sys import argv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import HALOModel\n",
    "from config import HALOConfig\n",
    "\n",
    "config = HALOConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HALOModel(config).to(device)\n",
    "checkpoint = torch.load(\"./save/generator_model\", map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "idxToId = pickle.load(open(\"discretized_data/idxToId.pkl\", \"rb\"))\n",
    "idToLab = pickle.load(open(\"discretized_data/idToLab.pkl\", \"rb\"))\n",
    "beginPos = pickle.load(open(\"discretized_data/beginPos.pkl\", \"rb\"))\n",
    "isCategorical = pickle.load(open(\"discretized_data/isCategorical.pkl\", \"rb\"))\n",
    "possible_values = pickle.load(open(\"discretized_data/possibleValues.pkl\", \"rb\"))\n",
    "discretization = pickle.load(open(\"discretized_data/discretization.pkl\", \"rb\"))\n",
    "\n",
    "def sample_sequence(model, length, context, batch_size, device=\"cuda\", sample=True):\n",
    "    empty = torch.zeros(\n",
    "        (1, 1, config.total_vocab_size), device=device, dtype=torch.float32\n",
    "    ).repeat(batch_size, 1, 1)\n",
    "    context = (\n",
    "        torch.tensor(context, device=device, dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .repeat(batch_size, 1)\n",
    "    )\n",
    "    prev = context.unsqueeze(1)\n",
    "    context = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - 1):\n",
    "            prev = model.sample(torch.cat((prev, empty), dim=1), sample)\n",
    "            if (\n",
    "                torch.sum(\n",
    "                    torch.sum(\n",
    "                        prev[\n",
    "                            :, :, config.code_vocab_size + config.label_vocab_size + 1\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    )\n",
    "                    .bool()\n",
    "                    .int(),\n",
    "                    dim=0,\n",
    "                ).item()\n",
    "                == batch_size\n",
    "            ):\n",
    "                break\n",
    "    ehr = prev.cpu().detach().numpy()\n",
    "    prev = None\n",
    "    empty = None\n",
    "    return ehr\n",
    "def convert_ehr(ehrs, index_to_code=None):\n",
    "    ehr_outputs = []\n",
    "    for i in range(len(ehrs)):\n",
    "        ehr = ehrs[i]\n",
    "        print(ehr)\n",
    "        ehr_output = []\n",
    "        ethnicity_output = ehr[3]\n",
    "        gender_output = ehr[2]\n",
    "        labels_output = ehr[1][\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size : config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "        ]\n",
    "        if index_to_code is not None:\n",
    "            labels_output = [idToLabel[idx] for idx in np.nonzero(labels_output)[0]]\n",
    "        \n",
    "         \n",
    "        for j in range(2, len(ehr)):\n",
    "            visit = ehr[j]\n",
    "            visit_output = []\n",
    "            lab_mask = []\n",
    "            lab_values = []\n",
    "            cont_idx = -1\n",
    "            indices = np.nonzero(visit)[0]\n",
    "            end = False\n",
    "            for idx in indices:\n",
    "                if idx < config.code_vocab_size:\n",
    "                    visit_output.append(\n",
    "                        index_to_code[idx] if index_to_code is not None else idx\n",
    "                    )\n",
    "                elif idx < config.code_vocab_size + config.lab_vocab_size:\n",
    "                    lab_idx = idx - (config.code_vocab_size)\n",
    "                    lab_num = idxToId[lab_idx]\n",
    "                    if lab_num in lab_mask:\n",
    "                        continue\n",
    "                    else:\n",
    "                        lab_mask.append(lab_num)\n",
    "                        lab_values.append(lab_idx - beginPos[lab_num])\n",
    "                elif (\n",
    "                    idx\n",
    "                    < config.code_vocab_size\n",
    "                    + config.lab_vocab_size\n",
    "                    + config.continuous_vocab_size\n",
    "                ):\n",
    "                    cont_idx = (\n",
    "                        cont_idx\n",
    "                        if cont_idx != -1\n",
    "                        else idx - (config.code_vocab_size + config.lab_vocab_size)\n",
    "                    )\n",
    "                elif (\n",
    "                    idx\n",
    "                    == config.code_vocab_size\n",
    "                    + config.lab_vocab_size\n",
    "                    + config.continuous_vocab_size\n",
    "                    + config.label_vocab_size\n",
    "                    + 1\n",
    "                ):\n",
    "                    end = True\n",
    "\n",
    "            if cont_idx == -1:\n",
    "                cont_idx = random.randint(0, config.continuous_vocab_size) - 1\n",
    "            if visit_output != [] or lab_mask != []:\n",
    "                ehr_output.append((visit_output, lab_mask, lab_values, [cont_idx]))\n",
    "            if end:\n",
    "                break\n",
    "\n",
    "        ehr_outputs.append({\"visits\": ehr_output, \"labels\": labels_output, \"gender\":gender_output,\"ethnicity\":ethnicity_output})\n",
    "    ehr = None\n",
    "    ehr_output = None\n",
    "    labels_output = None\n",
    "    visit = None\n",
    "    visit_output = None\n",
    "    indices = None\n",
    "    return ehr_outputs\n",
    "pakEHRs = pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\"))\n",
    "\n",
    "# Generate Synthetic EHR dataset\n",
    "# totEHRs = len(pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\")))\n",
    "totEHRs = 1000\n",
    "stoken = np.zeros(config.total_vocab_size)\n",
    "stoken[\n",
    "    config.code_vocab_size\n",
    "    + config.lab_vocab_size\n",
    "    + config.continuous_vocab_size\n",
    "    + config.label_vocab_size\n",
    "] = 1\n",
    "synthetic_ehr_dataset = []\n",
    "for i in tqdm(range(0, totEHRs, config.sample_batch_size)):\n",
    "    bs = min([totEHRs - i, config.sample_batch_size])\n",
    "    batch_synthetic_ehrs = sample_sequence(\n",
    "        model, config.n_ctx, stoken, batch_size=bs, device=device, sample=True\n",
    "    )\n",
    "    batch_synthetic_ehrs = convert_ehr(batch_synthetic_ehrs)\n",
    "    synthetic_ehr_dataset += batch_synthetic_ehrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9748c9-7fef-4313-82e6-a83fb6c0f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(synthetic_ehr_dataset, open(f\"./results/datasets/haloDataset_eth_1000.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdd6a3f-6c7f-421c-9169-b5cc82a69498",
   "metadata": {},
   "source": [
    "# Convert discrete data to continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c49864-80c0-4658-878c-31635479c56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "idToLab = pickle.load(open(\"discretized_data/idToLab.pkl\", \"rb\"))\n",
    "isCategorical = pickle.load(open(\"discretized_data/isCategorical.pkl\", \"rb\"))\n",
    "discretization = pickle.load(open(\"discretized_data/discretization.pkl\", \"rb\"))\n",
    "possibleValues = pickle.load(open(\"discretized_data/possibleValues.pkl\", \"rb\"))\n",
    "discretization = pickle.load(open(\"discretized_data/discretization.pkl\", \"rb\"))\n",
    "formatMap = pickle.load(open(\"discretized_data/formatMap.pkl\", \"rb\"))\n",
    "\n",
    "dataset = pickle.load(open(\"./results/datasets/haloDataset_feedback_2000.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "def formatCont(value, key):\n",
    "    return formatMap[key][1]((\"{:\" + formatMap[key][0] + \"}\").format(value))\n",
    "\n",
    "\n",
    "for p in dataset:\n",
    "    new_visits = []\n",
    "    firstVisit = True\n",
    "    for v in p[\"visits\"]:\n",
    "        new_labs = []\n",
    "        new_values = []\n",
    "        for i in range(len(v[1])):\n",
    "            new_labs.append(idToLab[v[1][i]])\n",
    "            if isCategorical[idToLab[v[1][i]]]:\n",
    "                new_values.append(possibleValues[idToLab[v[1][i]]][v[2][i]])\n",
    "            else:\n",
    "                new_values.append(\n",
    "                    formatCont(\n",
    "                        random.uniform(\n",
    "                            discretization[idToLab[v[1][i]]][v[2][i]],\n",
    "                            discretization[idToLab[v[1][i]]][v[2][i] + 1],\n",
    "                        ),\n",
    "                        idToLab[v[1][i]],\n",
    "                    )\n",
    "                )\n",
    "        contType = \"Hours\" if new_labs != [] else \"Age\" if firstVisit else \"Days\"\n",
    "        if contType == \"Age\":\n",
    "            firstVisit = False\n",
    "        new_cont = formatCont(\n",
    "            random.uniform(\n",
    "                discretization[contType][v[3][-1]],\n",
    "                discretization[contType][v[3][-1] + 1],\n",
    "            ),\n",
    "            contType,\n",
    "        )\n",
    "        new_visits.append((v[0], new_labs, new_values, [new_cont]))\n",
    "    p[\"visits\"] = new_visits\n",
    "\n",
    "pickle.dump(dataset, open(\"results/datasets/haloDataset_converted_feedback_2000.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56229ac-757a-4cf6-8e08-eaf4a87e3cdb",
   "metadata": {},
   "source": [
    "# Training the prediction model and calculating the fairness metrics\n",
    "\n",
    "The following cells are used for running the experiments described in the paper. Pyhealth library has been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18337a2-bda7-40eb-a419-d6341b4a3f74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5000 real + 1000 synth\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader, split_by_patient\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "\n",
    "all_data_5000 = pickle.load(open(f\"./data/allData_5000.pkl\", \"rb\"))\n",
    "halo_ehr_dataset = pickle.load(open(f\"./results/datasets/haloDataset_converted_2000.pkl\", \"rb\"))\n",
    "\n",
    "#all_data_2500 = random.sample(all_data_5000,2500)\n",
    "halo_1000 = random.sample(halo_ehr_dataset, 1000)\n",
    "\n",
    "combined_data = (\n",
    "    all_data_5000 + halo_1000\n",
    ")\n",
    "\n",
    "for patient in combined_data:\n",
    "    patient[\"labels\"] = [int(float(label)) for label in patient[\"labels\"].tolist()]\n",
    "\n",
    "def transform_data(ehr_dataset):\n",
    "    final_data = []\n",
    "    patient_id = 0  # Starting patient ID\n",
    "\n",
    "    for patient in ehr_dataset:\n",
    "        for i, visit in enumerate(patient[\"visits\"]):\n",
    "            visit_data = {\n",
    "                \"visit_id\": i,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"visit_codes\": [[int(x) for x in visit[0]]],\n",
    "                \"gender\": [[int(patient[\"labels\"][26])]],\n",
    "                \"ethnicity\": [[int(patient[\"labels\"][25])]],\n",
    "                \"disease_label\": [[int(x) for x in patient[\"labels\"][0:25]]],\n",
    "                \"label\": int(patient[\"labels\"][27]),\n",
    "            }\n",
    "            final_data.append(visit_data)\n",
    "        patient_id += 1\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def calculate_wtpr(y_true, y_prob, sensitive_attribute, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    subgroups = np.unique(sensitive_attribute)\n",
    "    tpr_scores = {}\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_mask = sensitive_attribute == subgroup\n",
    "        y_true_subgroup = y_true[subgroup_mask]\n",
    "        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "\n",
    "        confusion_mat = confusion_matrix(y_true_subgroup, y_pred_subgroup)\n",
    "\n",
    "        if confusion_mat.size == 1:\n",
    "            if y_true_subgroup[0] == 1:\n",
    "                tp = confusion_mat[0, 0]\n",
    "                fn = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fn = confusion_mat[0, 0]\n",
    "            tn = fp = 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        tpr_scores[subgroup] = tpr\n",
    "\n",
    "    wtpr = np.mean(list(tpr_scores.values()))\n",
    "    return wtpr, tpr_scores\n",
    "\n",
    "\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Calculate the split indices\n",
    "total_length = len(combined_data)\n",
    "train_split = int(0.8 * total_length)\n",
    "val_split = int(0.9 * total_length)\n",
    "\n",
    "# Split the combined list into train, validation, and test sets\n",
    "train_ehr_data = combined_data[:train_split]\n",
    "val_ehr_data = combined_data[train_split:val_split]\n",
    "test_ehr_data = combined_data[val_split:]\n",
    "\n",
    "transformed_train_ehr_dataset = transform_data(train_ehr_data)\n",
    "transformed_val_ehr_dataset = transform_data(val_ehr_data)\n",
    "transformed_test_ehr_dataset = transform_data(test_ehr_data)\n",
    "transformed_combined_ehr_dataset = transform_data(combined_data)\n",
    "\n",
    "max_visit_codes_length = max(\n",
    "    len(sample[\"visit_codes\"][0]) for sample in transformed_combined_ehr_dataset\n",
    ")\n",
    "for sample in transformed_train_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_val_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_test_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_combined_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "\n",
    "formatted_train_ehr_dataset = SampleEHRDataset(samples=transformed_train_ehr_dataset)\n",
    "formatted_val_ehr_dataset = SampleEHRDataset(samples=transformed_val_ehr_dataset)\n",
    "formatted_test_ehr_dataset = SampleEHRDataset(samples=transformed_test_ehr_dataset)\n",
    "formatted_combined_ehr_dataset = SampleEHRDataset(\n",
    "    samples=transformed_combined_ehr_dataset\n",
    ")\n",
    "\n",
    "# train_loader = get_dataloader(formatted_train_ehr_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = get_dataloader(formatted_val_ehr_dataset, batch_size=64, shuffle=False)\n",
    "# test_loader = get_dataloader(formatted_test_ehr_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "k = 5  # Number of folds\n",
    "fairness_scores = {\n",
    "    \"disparate_impact\": [],\n",
    "    \"statistical_parity_difference\": [],\n",
    "    \"wtpr\": [],\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "    fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "    fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "\n",
    "    transformermodel = Transformer(\n",
    "        dataset=formatted_train_ehr_dataset,\n",
    "        # look up what are available for \"feature_keys\" and \"label_keys\" in dataset.samples[0]\n",
    "        feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "        label_key=\"label\",\n",
    "        mode=\"binary\",\n",
    "    )\n",
    "\n",
    "    train_loader = get_dataloader(fold_train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = get_dataloader(fold_val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    trainer = Trainer(model=transformermodel)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=30,\n",
    "        monitor=\"pr_auc\",\n",
    "    )\n",
    "\n",
    "    y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "\n",
    "    # protected_group = 1  # female in gender\n",
    "    # # Prepare the sensitive attribute array for the validation set\n",
    "    # sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    # for idx, visit in enumerate(fold_val_dataset):\n",
    "    #     sensitive_attribute_value = visit[\"gender\"][0][0]\n",
    "    #     if sensitive_attribute_value == protected_group:\n",
    "    #         sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    #Prepare the sensitive attribute array for the validation set\n",
    " \n",
    "    unprotected_group = 0  # white in eth\n",
    "    sensitive_attribute_array= np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    for idx, visit in enumerate(fold_val_dataset):\n",
    "            sensitive_attribute_value = visit[\"ethnicity\"][0][0]\n",
    "            if sensitive_attribute_value != unprotected_group:\n",
    "                sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    \n",
    "    # Calculate fairness metrics for the current fold\n",
    "    fold_fairness_metrics = fairness_metrics_fn(\n",
    "        y_true,\n",
    "        y_prob,\n",
    "        sensitive_attributes=sensitive_attribute_array,\n",
    "        favorable_outcome=1,\n",
    "        metrics=None,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "\n",
    "    # Append the fairness metrics for the current fold\n",
    "    fairness_scores[\"disparate_impact\"].append(\n",
    "        fold_fairness_metrics[\"disparate_impact\"]\n",
    "    )\n",
    "    fairness_scores[\"statistical_parity_difference\"].append(\n",
    "        fold_fairness_metrics[\"statistical_parity_difference\"]\n",
    "    )\n",
    "    fairness_scores[\"wtpr\"].append(wtpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799486de-50e4-4c38-ad24-0744fb063bf5",
   "metadata": {},
   "source": [
    "# Training with feedback-generated metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ff27f-220d-4cef-9879-12abef9aab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 4\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "config = HALOConfig()\n",
    "\n",
    "local_rank = -1\n",
    "fp16 = False\n",
    "if local_rank == -1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "print(device)\n",
    "train_ehr_dataset = pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\"))\n",
    "val_ehr_dataset = pickle.load(open(\"discretized_data/valDataset.pkl\", \"rb\"))\n",
    "\n",
    "# Convert to fully codes\n",
    "beginPos = pickle.load(open(\"discretized_data/beginPos.pkl\", \"rb\"))\n",
    "for p in train_ehr_dataset + val_ehr_dataset:\n",
    "    new_visits = []\n",
    "    for v in p[\"visits\"]:\n",
    "        new_idx = v[0]\n",
    "        for l, val in zip(v[1], v[2]):\n",
    "            new_idx.append(config.code_vocab_size + beginPos[l] + val)\n",
    "        new_idx.append(config.code_vocab_size + config.lab_vocab_size + v[3][-1])\n",
    "        new_visits.append(new_idx)\n",
    "\n",
    "    p[\"visits\"] = new_visits\n",
    "\n",
    "\n",
    "def get_batch(loc, batch_size, mode):\n",
    "    if mode == \"train\":\n",
    "        ehr = train_ehr_dataset[loc : loc + batch_size]\n",
    "    elif mode == \"valid\":\n",
    "        ehr = val_ehr_dataset[loc : loc + batch_size]\n",
    "    else:\n",
    "        ehr = test_ehr_dataset[loc : loc + batch_size]\n",
    "\n",
    "    batch_gender = np.zeros((len(ehr)))\n",
    "    batch_ehr = np.zeros(\n",
    "        (len(ehr), config.n_ctx, config.total_vocab_size)\n",
    "    )  # 3d array len(ehr) * config.n_ctx * config.total_vocab_size\n",
    "    batch_mask = np.zeros(\n",
    "        (len(ehr), config.n_ctx, 1)\n",
    "    )  # 3d array len(ehr) * config.n_ctx * 1\n",
    "    for i, p in enumerate(ehr):\n",
    "        visits = p[\"visits\"]\n",
    "        for j, v in enumerate(visits):\n",
    "            batch_ehr[i, j + 2][v] = 1\n",
    "            batch_mask[i, j + 2] = 1\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size : config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size,\n",
    "        ] = np.array(\n",
    "            p[\"labels\"]\n",
    "        )  # Set the patient labels\n",
    "\n",
    "        batch_gender[i] = batch_ehr[\n",
    "            i,\n",
    "            1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            - 2,\n",
    "        ]\n",
    "        # batch_ethnicities[i] = batch_ehr[i,1,config.code_vocab_size + config.lab_vocab_size + config.continuous_vocab_size + config.label_vocab_size - 3]\n",
    "\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            len(visits) + 1,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            + 1,\n",
    "        ] = 1  # Set the final visit to have the end token\n",
    "        batch_ehr[\n",
    "            i,\n",
    "            len(visits) + 2 :,\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "            + 2,\n",
    "        ] = 1  # Set the rest to the padded visit token\n",
    "\n",
    "    batch_mask[:, 1] = 1  # Set the mask to cover the labels\n",
    "    batch_ehr[\n",
    "        :,\n",
    "        0,\n",
    "        config.code_vocab_size\n",
    "        + config.lab_vocab_size\n",
    "        + config.continuous_vocab_size\n",
    "        + config.label_vocab_size,\n",
    "    ] = 1  # Set the first visits to be the start token\n",
    "    batch_mask = batch_mask[\n",
    "        :, 1:, :\n",
    "    ]  # Shift the mask to match the shifted labels and predictions the model will return\n",
    "\n",
    "    return batch_ehr, batch_mask, batch_gender\n",
    "\n",
    "\n",
    "def shuffle_training_data(train_ehr_dataset):\n",
    "    np.random.shuffle(train_ehr_dataset)\n",
    "\n",
    "\n",
    "model = HALOModel(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
    "# if os.path.exists(\"./save/halo_model\"):\n",
    "#     print(\"Loading previous model\")\n",
    "#     checkpoint = torch.load(\"./save/halo_model\", map_location=torch.device(device))\n",
    "#     model.load_state_dict(checkpoint[\"model\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\n",
    "# Train Model\n",
    "global_loss = 1e10\n",
    "for e in tqdm(range(config.epoch)):\n",
    "    shuffle_training_data(train_ehr_dataset)\n",
    "    for i in range(0, len(train_ehr_dataset), config.batch_size):\n",
    "        model.train()\n",
    "\n",
    "        batch_ehr, batch_mask, batch_gender = get_batch(i, config.batch_size, \"train\")\n",
    "        batch_gender = torch.tensor(batch_gender, dtype=torch.float32).to(device)\n",
    "        batch_ehr = torch.tensor(batch_ehr, dtype=torch.float32).to(device)\n",
    "        batch_mask = torch.tensor(batch_mask, dtype=torch.float32).to(device)\n",
    "        #print(batch_mask.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss, _, _ = model(\n",
    "            batch_ehr,\n",
    "            batch_gender,\n",
    "            fairness_metrics,\n",
    "            input_ethnicities=None,\n",
    "            position_ids=None,\n",
    "            ehr_labels=batch_ehr,\n",
    "            ehr_masks=batch_mask,\n",
    "        )\n",
    "        # print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % (50 * config.batch_size) == 0:\n",
    "            print(\"Epoch %d, Iter %d: Training Loss:%.6f\" % (e, i, loss))\n",
    "        if i % (250 * config.batch_size) == 0:\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                val_l = []\n",
    "                for v_i in range(0, len(val_ehr_dataset), config.batch_size):\n",
    "                    batch_ehr, batch_mask, batch_gender = get_batch(\n",
    "                        i, config.batch_size, \"train\"\n",
    "                    )\n",
    "                    batch_gender = torch.tensor(batch_gender, dtype=torch.float32).to(\n",
    "                        device\n",
    "                    )\n",
    "                    # batch_ethnicities = torch.tensor(batch_ethnicities,dtype=torch.float32).to(device)\n",
    "                    batch_ehr = torch.tensor(batch_ehr, dtype=torch.float32).to(device)\n",
    "                    batch_mask = torch.tensor(batch_mask, dtype=torch.float32).to(\n",
    "                        device\n",
    "                    )\n",
    "\n",
    "                    val_loss, _, _ = model(\n",
    "                        batch_ehr,\n",
    "                        batch_gender,\n",
    "                        fairness_metrics,\n",
    "                        input_ethnicities=None,\n",
    "                        position_ids=None,\n",
    "                        ehr_labels=batch_ehr,\n",
    "                        ehr_masks=batch_mask,\n",
    "                    )\n",
    "                    val_l.append((val_loss).cpu().detach().numpy())\n",
    "\n",
    "                cur_val_loss = np.mean(val_l)\n",
    "                print(\"Epoch %d Validation Loss:%.7f\" % (e, cur_val_loss))\n",
    "                if cur_val_loss < global_loss:\n",
    "                    global_loss = cur_val_loss\n",
    "                    state = {\n",
    "                        \"model\": model.state_dict(),\n",
    "                        \"optimizer\": optimizer.state_dict(),\n",
    "                        \"iteration\": i,\n",
    "                    }\n",
    "                    torch.save(state, \"./save/generated_model_from_downstream\")\n",
    "                    print(\"\\n------------ Save best model ------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bd1b3-2cac-4ed7-94d0-668b14712431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "config = HALOConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HALOModel(config).to(device)\n",
    "checkpoint = torch.load(\"./save/generated_model_from_downstream\", map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "idxToId = pickle.load(open(\"discretized_data/idxToId.pkl\", \"rb\"))\n",
    "idToLab = pickle.load(open(\"discretized_data/idToLab.pkl\", \"rb\"))\n",
    "beginPos = pickle.load(open(\"discretized_data/beginPos.pkl\", \"rb\"))\n",
    "isCategorical = pickle.load(open(\"discretized_data/isCategorical.pkl\", \"rb\"))\n",
    "possible_values = pickle.load(open(\"discretized_data/possibleValues.pkl\", \"rb\"))\n",
    "discretization = pickle.load(open(\"discretized_data/discretization.pkl\", \"rb\"))\n",
    "\n",
    "def sample_sequence(model, length, context, batch_size, device=\"cuda\", sample=True):\n",
    "    empty = torch.zeros(\n",
    "        (1, 1, config.total_vocab_size), device=device, dtype=torch.float32\n",
    "    ).repeat(batch_size, 1, 1)\n",
    "    context = (\n",
    "        torch.tensor(context, device=device, dtype=torch.float32)\n",
    "        .unsqueeze(0)\n",
    "        .repeat(batch_size, 1)\n",
    "    )\n",
    "    prev = context.unsqueeze(1)\n",
    "    context = None\n",
    "    with torch.no_grad():\n",
    "        for _ in range(length - 1):\n",
    "            prev = model.sample(torch.cat((prev, empty), dim=1), sample)\n",
    "            if (\n",
    "                torch.sum(\n",
    "                    torch.sum(\n",
    "                        prev[\n",
    "                            :, :, config.code_vocab_size + config.label_vocab_size + 1\n",
    "                        ],\n",
    "                        dim=1,\n",
    "                    )\n",
    "                    .bool()\n",
    "                    .int(),\n",
    "                    dim=0,\n",
    "                ).item()\n",
    "                == batch_size\n",
    "            ):\n",
    "                break\n",
    "    ehr = prev.cpu().detach().numpy()\n",
    "    prev = None\n",
    "    empty = None\n",
    "    return ehr\n",
    "def convert_ehr(ehrs, index_to_code=None):\n",
    "    ehr_outputs = []\n",
    "    for i in range(len(ehrs)):\n",
    "        ehr = ehrs[i]\n",
    "        print(ehr)\n",
    "        ehr_output = []\n",
    "        ethnicity_output = ehr[3]\n",
    "        gender_output = ehr[2]\n",
    "        labels_output = ehr[1][\n",
    "            config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size : config.code_vocab_size\n",
    "            + config.lab_vocab_size\n",
    "            + config.continuous_vocab_size\n",
    "            + config.label_vocab_size\n",
    "        ]\n",
    "        if index_to_code is not None:\n",
    "            labels_output = [idToLabel[idx] for idx in np.nonzero(labels_output)[0]]\n",
    "        \n",
    "         \n",
    "        for j in range(2, len(ehr)):\n",
    "            visit = ehr[j]\n",
    "            visit_output = []\n",
    "            lab_mask = []\n",
    "            lab_values = []\n",
    "            cont_idx = -1\n",
    "            indices = np.nonzero(visit)[0]\n",
    "            end = False\n",
    "            for idx in indices:\n",
    "                if idx < config.code_vocab_size:\n",
    "                    visit_output.append(\n",
    "                        index_to_code[idx] if index_to_code is not None else idx\n",
    "                    )\n",
    "                elif idx < config.code_vocab_size + config.lab_vocab_size:\n",
    "                    lab_idx = idx - (config.code_vocab_size)\n",
    "                    lab_num = idxToId[lab_idx]\n",
    "                    if lab_num in lab_mask:\n",
    "                        continue\n",
    "                    else:\n",
    "                        lab_mask.append(lab_num)\n",
    "                        lab_values.append(lab_idx - beginPos[lab_num])\n",
    "                elif (\n",
    "                    idx\n",
    "                    < config.code_vocab_size\n",
    "                    + config.lab_vocab_size\n",
    "                    + config.continuous_vocab_size\n",
    "                ):\n",
    "                    cont_idx = (\n",
    "                        cont_idx\n",
    "                        if cont_idx != -1\n",
    "                        else idx - (config.code_vocab_size + config.lab_vocab_size)\n",
    "                    )\n",
    "                elif (\n",
    "                    idx\n",
    "                    == config.code_vocab_size\n",
    "                    + config.lab_vocab_size\n",
    "                    + config.continuous_vocab_size\n",
    "                    + config.label_vocab_size\n",
    "                    + 1\n",
    "                ):\n",
    "                    end = True\n",
    "\n",
    "            if cont_idx == -1:\n",
    "                cont_idx = random.randint(0, config.continuous_vocab_size) - 1\n",
    "            if visit_output != [] or lab_mask != []:\n",
    "                ehr_output.append((visit_output, lab_mask, lab_values, [cont_idx]))\n",
    "            if end:\n",
    "                break\n",
    "\n",
    "        ehr_outputs.append({\"visits\": ehr_output, \"labels\": labels_output, \"gender\":gender_output,\"ethnicity\":ethnicity_output})\n",
    "    ehr = None\n",
    "    ehr_output = None\n",
    "    labels_output = None\n",
    "    visit = None\n",
    "    visit_output = None\n",
    "    indices = None\n",
    "    return ehr_outputs\n",
    "pakEHRs = pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\"))\n",
    "\n",
    "# Generate Synthetic EHR dataset\n",
    "# totEHRs = len(pickle.load(open(\"discretized_data/trainDataset.pkl\", \"rb\")))\n",
    "totEHRs = 2000\n",
    "stoken = np.zeros(config.total_vocab_size)\n",
    "stoken[\n",
    "    config.code_vocab_size\n",
    "    + config.lab_vocab_size\n",
    "    + config.continuous_vocab_size\n",
    "    + config.label_vocab_size\n",
    "] = 1\n",
    "synthetic_ehr_dataset = []\n",
    "for i in tqdm(range(0, totEHRs, config.sample_batch_size)):\n",
    "    bs = min([totEHRs - i, config.sample_batch_size])\n",
    "    batch_synthetic_ehrs = sample_sequence(\n",
    "        model, config.n_ctx, stoken, batch_size=bs, device=device, sample=True\n",
    "    )\n",
    "    batch_synthetic_ehrs = convert_ehr(batch_synthetic_ehrs)\n",
    "    synthetic_ehr_dataset += batch_synthetic_ehrs\n",
    "pickle.dump(synthetic_ehr_dataset, open(f\"./results/datasets/haloDataset_downstream_optimized.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecff86c-5548-4eed-a815-7d42f9c01ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_ehr_dataset = pickle.load(open(f\"./data/trainData.pkl\", \"rb\"))\n",
    "val_ehr_dataset = pickle.load(open(f\"./data/valData.pkl\", \"rb\"))\n",
    "test_ehr_dataset = pickle.load(open(f\"./data/testData.pkl\", \"rb\"))\n",
    "halo_ehr_dataset = pickle.load(\n",
    "    open(f\"./results/datasets/haloDataset_downstream_optimized.pkl\", \"rb\")\n",
    ")\n",
    "\n",
    "combined_data = (\n",
    "    train_ehr_dataset + val_ehr_dataset + test_ehr_dataset + halo_ehr_dataset\n",
    ")\n",
    "\n",
    "for patient in combined_data:\n",
    "    patient[\"labels\"] = [int(float(label)) for label in patient[\"labels\"].tolist()]\n",
    "\n",
    "def transform_data(ehr_dataset):\n",
    "    final_data = []\n",
    "    patient_id = 0  # Starting patient ID\n",
    "\n",
    "    for patient in ehr_dataset:\n",
    "        for i, visit in enumerate(patient[\"visits\"]):\n",
    "            visit_data = {\n",
    "                \"visit_id\": i,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"visit_codes\": [[int(x) for x in visit[0]]],\n",
    "                \"gender\": [[int(patient[\"labels\"][26])]],\n",
    "                \"ethnicity\": [[int(patient[\"labels\"][25])]],\n",
    "                \"disease_label\": [[int(x) for x in patient[\"labels\"][0:25]]],\n",
    "                \"label\": int(patient[\"labels\"][27]),\n",
    "            }\n",
    "            final_data.append(visit_data)\n",
    "        patient_id += 1\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def calculate_wtpr(y_true, y_pred, sensitive_attribute):\n",
    "    subgroups = np.unique(sensitive_attribute)\n",
    "    tpr_scores = {}\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_mask = sensitive_attribute == subgroup\n",
    "        y_true_subgroup = y_true[subgroup_mask]\n",
    "        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_subgroup, y_pred_subgroup).ravel()\n",
    "        tpr = tp / (tp + fn)\n",
    "        tpr_scores[subgroup] = tpr\n",
    "\n",
    "    wtpr = min(tpr_scores.values())\n",
    "\n",
    "    print(\"TPR scores for each subgroup:\")\n",
    "    for subgroup, tpr in tpr_scores.items():\n",
    "        print(f\"Subgroup {subgroup}: TPR = {tpr:.3f}\")\n",
    "\n",
    "    return wtpr\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader, split_by_patient\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "# Calculate the split indices\n",
    "total_length = len(combined_data)\n",
    "train_split = int(0.8 * total_length)\n",
    "val_split = int(0.9 * total_length)\n",
    "\n",
    "# Split the combined list into train, validation, and test sets\n",
    "train_ehr_data = combined_data[:train_split]\n",
    "val_ehr_data = combined_data[train_split:val_split]\n",
    "test_ehr_data = combined_data[val_split:]\n",
    "\n",
    "transformed_train_ehr_dataset = transform_data(train_ehr_data)\n",
    "transformed_val_ehr_dataset = transform_data(val_ehr_data)\n",
    "transformed_test_ehr_dataset = transform_data(test_ehr_data)\n",
    "transformed_combined_ehr_dataset = transform_data(combined_data)\n",
    "\n",
    "max_visit_codes_length = max(\n",
    "    len(sample[\"visit_codes\"][0]) for sample in transformed_combined_ehr_dataset\n",
    ")\n",
    "for sample in transformed_train_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_val_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_test_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "for sample in transformed_combined_ehr_dataset:\n",
    "    visit_codes = sample[\"visit_codes\"][0]\n",
    "    padded_visit_codes = np.pad(\n",
    "        visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "    )\n",
    "    sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "\n",
    "formatted_train_ehr_dataset = SampleEHRDataset(samples=transformed_train_ehr_dataset)\n",
    "formatted_val_ehr_dataset = SampleEHRDataset(samples=transformed_val_ehr_dataset)\n",
    "formatted_test_ehr_dataset = SampleEHRDataset(samples=transformed_test_ehr_dataset)\n",
    "formatted_combined_ehr_dataset = SampleEHRDataset(\n",
    "    samples=transformed_combined_ehr_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec6c200-5efd-4edf-b000-22d8225c6ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader\n",
    "from pyhealth.metrics.binary import binary_metrics_fn\n",
    "from pyhealth.metrics.fairness import fairness_metrics_fn\n",
    "from pyhealth.models import RNN, Transformer\n",
    "from pyhealth.trainer import Trainer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 5  # Number of folds\n",
    "fairness_scores = {\n",
    "    \"disparate_impact\": [],\n",
    "    \"statistical_parity_difference\": [],\n",
    "    \"wtpr\": [],\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "    fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "    fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "\n",
    "    transformermodel = Transformer(\n",
    "        dataset=formatted_train_ehr_dataset,\n",
    "        # look up what are available for \"feature_keys\" and \"label_keys\" in dataset.samples[0]\n",
    "        feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "        label_key=\"label\",\n",
    "        mode=\"binary\",\n",
    "    )\n",
    "\n",
    "    train_loader = get_dataloader(fold_train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = get_dataloader(fold_val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    trainer = Trainer(model=transformermodel)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=50,\n",
    "        monitor=\"pr_auc\",\n",
    "    )\n",
    "\n",
    "    y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "\n",
    "    protected_group = 1  # female in gender\n",
    "    # Prepare the sensitive attribute array for the validation set\n",
    "    sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    for idx, visit in enumerate(fold_val_dataset):\n",
    "        sensitive_attribute_value = visit[\"gender\"][0][0]\n",
    "        if sensitive_attribute_value == protected_group:\n",
    "            sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    # Calculate fairness metrics for the current fold\n",
    "    fold_fairness_metrics = fairness_metrics_fn(\n",
    "        y_true,\n",
    "        y_prob,\n",
    "        sensitive_attributes=sensitive_attribute_array,\n",
    "        favorable_outcome=1,\n",
    "        metrics=None,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "\n",
    "    # Append the fairness metrics for the current fold\n",
    "    fairness_scores[\"disparate_impact\"].append(\n",
    "        fold_fairness_metrics[\"disparate_impact\"]\n",
    "    )\n",
    "    fairness_scores[\"statistical_parity_difference\"].append(\n",
    "        fold_fairness_metrics[\"statistical_parity_difference\"]\n",
    "    )\n",
    "    fairness_scores[\"wtpr\"].append(wtpr)\n",
    "\n",
    "# Calculate the mean and standard deviation for each fairness metric\n",
    "for metric, scores in fairness_scores.items():\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    print(f\"{metric}: Mean = {mean:.4f}, Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c605f5c-7d35-4621-b3bc-166ddf1bca49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyhealth.models import RNN\n",
    "\n",
    "k = 5  # Number of folds\n",
    "fairness_scores = []\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fairness_scores = {\n",
    "    \"disparate_impact\": [],\n",
    "    \"statistical_parity_difference\": [],\n",
    "    \"wtpr\": [],\n",
    "}\n",
    "\n",
    "for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "    fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "    fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "\n",
    "    rnnModel = RNN(\n",
    "        dataset=formatted_train_ehr_dataset,\n",
    "        # look up what are available for \"feature_keys\" and \"label_keys\" in dataset.samples[0]\n",
    "        feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "        label_key=\"label\",\n",
    "        mode=\"binary\",\n",
    "    )\n",
    "\n",
    "    train_loader = get_dataloader(fold_train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = get_dataloader(fold_val_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "    trainer = Trainer(model=rnnModel)\n",
    "    trainer.train(\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        epochs=50,\n",
    "        monitor=\"pr_auc\",\n",
    "    )\n",
    "\n",
    "    y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "\n",
    "    protected_group = 1  # female in gender\n",
    "    # Prepare the sensitive attribute array for the validation set\n",
    "    sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "    for idx, visit in enumerate(fold_val_dataset):\n",
    "        sensitive_attribute_value = visit[\"gender\"][0][0]\n",
    "        if sensitive_attribute_value == protected_group:\n",
    "            sensitive_attribute_array[idx] = 1\n",
    "\n",
    "    # Calculate fairness metrics for the current fold\n",
    "    fold_fairness_metrics = fairness_metrics_fn(\n",
    "        y_true,\n",
    "        y_prob,\n",
    "        sensitive_attributes=sensitive_attribute_array,\n",
    "        favorable_outcome=1,\n",
    "        metrics=None,\n",
    "        threshold=0.5,\n",
    "    )\n",
    "    wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "\n",
    "    # Append the fairness metrics for the current fold\n",
    "    fairness_scores[\"disparate_impact\"].append(\n",
    "        fold_fairness_metrics[\"disparate_impact\"]\n",
    "    )\n",
    "    fairness_scores[\"statistical_parity_difference\"].append(\n",
    "        fold_fairness_metrics[\"statistical_parity_difference\"]\n",
    "    )\n",
    "    fairness_scores[\"wtpr\"].append(wtpr)\n",
    "    print(fairness_scores)\n",
    "\n",
    "# Calculate the mean and standard deviation for each fairness metric\n",
    "for metric, scores in fairness_scores.items():\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    print(f\"{metric}: Mean = {mean:.4f}, Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf549549-3ca7-4067-a6ff-a8a25976c972",
   "metadata": {},
   "source": [
    "# Real Data +Double Prioritized Bias Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1307afc-face-4aec-b820-a60bdaf01bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51087c-48ef-47be-88b7-d6e91bb7d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allData_5000 = pickle.load(open(\"data\\\\allData_5000.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2072977-fbe9-4b4d-8300-aad30aa72d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store the counts of each ethnicity\n",
    "ethnicity_counts = {}\n",
    "\n",
    "# Iterate over each patient visit dictionary in allData_5000\n",
    "for patient_visit in allData_5000:\n",
    "    # Extract the ethnicity value from the labels list\n",
    "    ethnicity = patient_visit['labels'][25]  # Assuming ethnicity is at index 25\n",
    "    \n",
    "    # Update the count for the corresponding ethnicity\n",
    "    if ethnicity in ethnicity_counts:\n",
    "        ethnicity_counts[ethnicity] += 1\n",
    "    else:\n",
    "        ethnicity_counts[ethnicity] = 1\n",
    "\n",
    "# Print the counts of each ethnicity\n",
    "for ethnicity, count in ethnicity_counts.items():\n",
    "    print(f\"Ethnicity {ethnicity}: {count} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbfff90-4bbd-48be-8aca-947aa4955c16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the number of additional patients needed for ethnicity 2 and 3\n",
    "additional_patients_2 = 100\n",
    "additional_patients_3 = 500\n",
    "additional_patients_4 = 400\n",
    "\n",
    "oversampled_1000 = []\n",
    "# Duplicate patient visits with ethnicity 2 or 3 until the desired count is reached\n",
    "while additional_patients_2 > 0 or additional_patients_3 > 0 or additional_patients_4 > 0:\n",
    "    patient_visit = random.choice(allData_5000)\n",
    "    ethnicity = patient_visit['labels'][25]  # Assuming ethnicity is at index 25\n",
    "    \n",
    "    if ethnicity == '3.0' and additional_patients_3 > 0:\n",
    "        # Duplicate the patient visit and add it to allData_5000\n",
    "        oversampled_1000.append(patient_visit)\n",
    "        additional_patients_3 -= 1\n",
    "        print(f\"add_pat_3: {additional_patients_3}\")\n",
    "    elif ethnicity == '4.0' and additional_patients_4 > 0:\n",
    "        # Duplicate the patient visit and add it to allData_5000\n",
    "        oversampled_1000.append(patient_visit)\n",
    "        additional_patients_4 -= 1\n",
    "        print(f\"add_pat_4: {additional_patients_4}\")\n",
    "    elif ethnicity == '2.0' and additional_patients_2 > 0:\n",
    "        oversampled_1000.append(patient_visit)\n",
    "        additional_patients_2 -= 1\n",
    "        print(f\"add_pat_2: {additional_patients_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c749e3be-ca64-450d-8fbd-885436d49fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the updated counts of each ethnicity\n",
    "ethnicity_counts = {}\n",
    "for patient_visit in oversampled_1000:\n",
    "    ethnicity = patient_visit['labels'][25]  # Assuming ethnicity is at index 25\n",
    "    \n",
    "    if ethnicity in ethnicity_counts:\n",
    "        ethnicity_counts[ethnicity] += 1\n",
    "    else:\n",
    "        ethnicity_counts[ethnicity] = 1\n",
    "\n",
    "for ethnicity, count in ethnicity_counts.items():\n",
    "    print(f\"Ethnicity {ethnicity}: {count} patients\")\n",
    "print(f\"TOTAL: {len(oversampled_1000)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7bf753-f81a-41e7-925f-e1f5dd46ac52",
   "metadata": {},
   "source": [
    "# Training Prediction Model with the Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c8701d-94ad-4d2d-97d2-03eee3073ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from pyhealth.datasets import SampleEHRDataset, get_dataloader, split_by_patient\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "\n",
    "def transform_data(ehr_dataset):\n",
    "    final_data = []\n",
    "    patient_id = 0  # Starting patient ID\n",
    "\n",
    "    for patient in ehr_dataset:\n",
    "        for i, visit in enumerate(patient[\"visits\"]):\n",
    "            visit_data = {\n",
    "                \"visit_id\": i,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"visit_codes\": [[int(x) for x in visit[0]]],\n",
    "                \"gender\": [[int(patient[\"labels\"][26])]],\n",
    "                \"ethnicity\": [[int(patient[\"labels\"][25])]],\n",
    "                \"disease_label\": [[int(x) for x in patient[\"labels\"][0:25]]],\n",
    "                \"label\": int(patient[\"labels\"][27]),\n",
    "            }\n",
    "            final_data.append(visit_data)\n",
    "        patient_id += 1\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def calculate_wtpr(y_true, y_prob, sensitive_attribute, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    subgroups = np.unique(sensitive_attribute)\n",
    "    tpr_scores = {}\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_mask = sensitive_attribute == subgroup\n",
    "        y_true_subgroup = y_true[subgroup_mask]\n",
    "        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "\n",
    "        confusion_mat = confusion_matrix(y_true_subgroup, y_pred_subgroup)\n",
    "\n",
    "        if confusion_mat.size == 1:\n",
    "            if y_true_subgroup[0] == 1:\n",
    "                tp = confusion_mat[0, 0]\n",
    "                fn = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fn = confusion_mat[0, 0]\n",
    "            tn = fp = 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        tpr_scores[subgroup] = tpr\n",
    "\n",
    "    wtpr = np.mean(list(tpr_scores.values()))\n",
    "    return wtpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600f750-49e1-4419-9610-0024409a10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(oversampled_1000, open(f\"./results/datasets/oversampled_1000.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43f228e-8d58-4c3d-a0e7-f0dc48d09fe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#5000 real + 1000 synth\n",
    "all_data_5000 = pickle.load(open(f\"./data/allData_5000.pkl\", \"rb\"))\n",
    "\n",
    "allData_1000 = random.sample(all_data_5000, 1000)\n",
    "allData_2500 = random.sample(all_data_5000, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee8c39-b203-45dd-bb39-0a39e7ef0f7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_datasets = [allData_2500]\n",
    "for real_dataset in real_datasets:\n",
    "    combined_data = (real_dataset + oversampled_1000)\n",
    "    \n",
    "    for patient in combined_data:\n",
    "        patient[\"labels\"] = [int(float(label)) for label in patient[\"labels\"]]#.tolist()]\n",
    "    \n",
    "    random.shuffle(combined_data)\n",
    "    \n",
    "    # Calculate the split indices\n",
    "    total_length = len(combined_data)\n",
    "    train_split = int(0.8 * total_length)\n",
    "    val_split = int(0.9 * total_length)\n",
    "    \n",
    "    # Split the combined list into train, validation, and test sets\n",
    "    train_ehr_data = combined_data[:train_split]\n",
    "    val_ehr_data = combined_data[train_split:val_split]\n",
    "    test_ehr_data = combined_data[val_split:]\n",
    "    \n",
    "    transformed_train_ehr_dataset = transform_data(train_ehr_data)\n",
    "    transformed_val_ehr_dataset = transform_data(val_ehr_data)\n",
    "    transformed_test_ehr_dataset = transform_data(test_ehr_data)\n",
    "    transformed_combined_ehr_dataset = transform_data(combined_data)\n",
    "    \n",
    "    max_visit_codes_length = max(\n",
    "        len(sample[\"visit_codes\"][0]) for sample in transformed_combined_ehr_dataset\n",
    "    )\n",
    "    for sample in transformed_train_ehr_dataset:\n",
    "        visit_codes = sample[\"visit_codes\"][0]\n",
    "        padded_visit_codes = np.pad(\n",
    "            visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "        )\n",
    "        sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "    for sample in transformed_val_ehr_dataset:\n",
    "        visit_codes = sample[\"visit_codes\"][0]\n",
    "        padded_visit_codes = np.pad(\n",
    "            visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "        )\n",
    "        sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "    for sample in transformed_test_ehr_dataset:\n",
    "        visit_codes = sample[\"visit_codes\"][0]\n",
    "        padded_visit_codes = np.pad(\n",
    "            visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "        )\n",
    "        sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "    for sample in transformed_combined_ehr_dataset:\n",
    "        visit_codes = sample[\"visit_codes\"][0]\n",
    "        padded_visit_codes = np.pad(\n",
    "            visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "        )\n",
    "        sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "    \n",
    "    formatted_train_ehr_dataset = SampleEHRDataset(samples=transformed_train_ehr_dataset)\n",
    "    formatted_val_ehr_dataset = SampleEHRDataset(samples=transformed_val_ehr_dataset)\n",
    "    formatted_test_ehr_dataset = SampleEHRDataset(samples=transformed_test_ehr_dataset)\n",
    "    formatted_combined_ehr_dataset = SampleEHRDataset(\n",
    "        samples=transformed_combined_ehr_dataset\n",
    "    )\n",
    "    \n",
    "    k = 5  # Number of folds\n",
    "    fairness_scores = {\n",
    "        \"disparate_impact\": [],\n",
    "        \"statistical_parity_difference\": [],\n",
    "        \"wtpr\": [],\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "        fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "        fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "    \n",
    "        transformermodel = Transformer(\n",
    "            dataset=formatted_train_ehr_dataset,\n",
    "            # look up what are available for \"feature_keys\" and \"label_keys\" in dataset.samples[0]\n",
    "            feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "            label_key=\"label\",\n",
    "            mode=\"binary\",\n",
    "        )\n",
    "    \n",
    "        train_loader = get_dataloader(fold_train_dataset, batch_size=64, shuffle=True)\n",
    "        val_loader = get_dataloader(fold_val_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "        trainer = Trainer(model=transformermodel)\n",
    "        trainer.train(\n",
    "            train_dataloader=train_loader,\n",
    "            val_dataloader=val_loader,\n",
    "            epochs=30,\n",
    "            monitor=\"pr_auc\",\n",
    "        )\n",
    "    \n",
    "        y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "    \n",
    "        # protected_group = 1  # female in gender\n",
    "        # # Prepare the sensitive attribute array for the validation set\n",
    "        # sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "        # for idx, visit in enumerate(fold_val_dataset):\n",
    "        #     sensitive_attribute_value = visit[\"gender\"][0][0]\n",
    "        #     if sensitive_attribute_value == protected_group:\n",
    "        #         sensitive_attribute_array[idx] = 1\n",
    "    \n",
    "        #Prepare the sensitive attribute array for the validation set\n",
    "     \n",
    "        unprotected_group = 0  # white in eth\n",
    "        sensitive_attribute_array= np.zeros(len(fold_val_dataset), dtype=int)\n",
    "        for idx, visit in enumerate(fold_val_dataset):\n",
    "                sensitive_attribute_value = visit[\"ethnicity\"][0][0]\n",
    "                if sensitive_attribute_value != unprotected_group:\n",
    "                    sensitive_attribute_array[idx] = 1\n",
    "    \n",
    "        \n",
    "        # Calculate fairness metrics for the current fold\n",
    "        fold_fairness_metrics = fairness_metrics_fn(\n",
    "            y_true,\n",
    "            y_prob,\n",
    "            sensitive_attributes=sensitive_attribute_array,\n",
    "            favorable_outcome=1,\n",
    "            metrics=None,\n",
    "            threshold=0.5,\n",
    "        )\n",
    "        wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "    \n",
    "        # Append the fairness metrics for the current fold\n",
    "        fairness_scores[\"disparate_impact\"].append(\n",
    "            fold_fairness_metrics[\"disparate_impact\"]\n",
    "        )\n",
    "        fairness_scores[\"statistical_parity_difference\"].append(\n",
    "            fold_fairness_metrics[\"statistical_parity_difference\"]\n",
    "        )\n",
    "        fairness_scores[\"wtpr\"].append(wtpr)\n",
    "    for metric, scores in fairness_scores.items():\n",
    "        mean = np.mean(scores)\n",
    "        std = np.std(scores)\n",
    "        print(f\"{metric}: Mean = {mean:.4f}, Std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fecc3b-2017-4039-a6ac-32cc83d99be5",
   "metadata": {},
   "source": [
    "# Experiments with Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648d704-8d00-4270-8aff-6bece7ae72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(ehr_dataset):\n",
    "    final_data = []\n",
    "    patient_id = 0  # Starting patient ID\n",
    "\n",
    "    for patient in ehr_dataset:\n",
    "        for i, visit in enumerate(patient[\"visits\"]):\n",
    "            visit_data = {\n",
    "                \"visit_id\": i,\n",
    "                \"patient_id\": patient_id,\n",
    "                \"visit_codes\": [[int(x) for x in visit[0]]],\n",
    "                \"gender\": [[int(float(patient[\"labels\"][26]))]],\n",
    "                \"ethnicity\": [[int(float(patient[\"labels\"][25]))]],\n",
    "                \"disease_label\": [[int(float(x)) for x in patient[\"labels\"][0:25]]],\n",
    "                \"label\": int(float(patient[\"labels\"][27])),\n",
    "            }\n",
    "            final_data.append(visit_data)\n",
    "        patient_id += 1\n",
    "    return final_data\n",
    "\n",
    "def calculate_wtpr(y_true, y_prob, sensitive_attribute, threshold=0.5):\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    subgroups = np.unique(sensitive_attribute)\n",
    "    tpr_scores = {}\n",
    "\n",
    "    for subgroup in subgroups:\n",
    "        subgroup_mask = sensitive_attribute == subgroup\n",
    "        y_true_subgroup = y_true[subgroup_mask]\n",
    "        y_pred_subgroup = y_pred[subgroup_mask]\n",
    "\n",
    "        confusion_mat = confusion_matrix(y_true_subgroup, y_pred_subgroup)\n",
    "\n",
    "        if confusion_mat.size == 1:\n",
    "            if y_true_subgroup[0] == 1:\n",
    "                tp = confusion_mat[0, 0]\n",
    "                fn = 0\n",
    "            else:\n",
    "                tp = 0\n",
    "                fn = confusion_mat[0, 0]\n",
    "            tn = fp = 0\n",
    "        else:\n",
    "            tn, fp, fn, tp = confusion_mat.ravel()\n",
    "\n",
    "        tpr = tp / (tp + fn) if (tp + fn) != 0 else 0\n",
    "        tpr_scores[subgroup] = tpr\n",
    "\n",
    "    wtpr = np.mean(list(tpr_scores.values()))\n",
    "    return wtpr\n",
    "\n",
    "def formatter(data, max_visit_codes_length):\n",
    "    for patient in data:\n",
    "        patient[\"labels\"] = [int(float(label)) for label in patient[\"labels\"]]\n",
    "    random.shuffle(data)\n",
    "    transformed_data = transform_data(data)\n",
    "    for sample in transformed_data:\n",
    "        visit_codes = sample[\"visit_codes\"][0]\n",
    "        padded_visit_codes = np.pad(\n",
    "            visit_codes, (0, max_visit_codes_length - len(visit_codes)), mode=\"constant\"\n",
    "        )\n",
    "        sample[\"visit_codes\"][0] = padded_visit_codes.tolist()\n",
    "    return SampleEHRDataset(samples=transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a872aa2-8fdb-4080-a2b1-9d5d230356d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_acc_calc(fixed_dataset, var_datasets, num_epochs):\n",
    "    for var_dataset in var_datasets:\n",
    "\n",
    "        f1_scores = []\n",
    "        fairness_scores = {\n",
    "            \"disparate_impact\": [],\n",
    "            \"wtpr\": [],\n",
    "        }\n",
    "        combined_data = fixed_dataset + var_dataset\n",
    "        train_ehr_data = combined_data[:int(0.8 * len(combined_data))]\n",
    "        max_visit_codes_length = max(len(sample[\"visit_codes\"][0]) for sample in transform_data(combined_data))\n",
    "        formatted_combined_ehr_dataset = formatter(combined_data, max_visit_codes_length)\n",
    "        formatted_train_ehr_dataset = formatter(train_ehr_data, max_visit_codes_length)\n",
    "\n",
    "        k = 5  # Number of folds\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "        for train_index, val_index in kf.split(formatted_combined_ehr_dataset):\n",
    "            fold_train_dataset = [formatted_combined_ehr_dataset[i] for i in train_index]\n",
    "            fold_val_dataset = [formatted_combined_ehr_dataset[i] for i in val_index]\n",
    "\n",
    "            transformermodel = Transformer(\n",
    "                dataset=formatted_train_ehr_dataset,\n",
    "                feature_keys=[\"visit_codes\", \"disease_label\", \"ethnicity\", \"gender\"],\n",
    "                label_key=\"label\",\n",
    "                mode=\"binary\",\n",
    "            )\n",
    "            train_loader = get_dataloader(fold_train_dataset, batch_size=32, shuffle=True)\n",
    "            val_loader = get_dataloader(fold_val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "            trainer = Trainer(model=transformermodel)\n",
    "            trainer.train(\n",
    "                train_dataloader=train_loader,\n",
    "                val_dataloader=val_loader,\n",
    "                epochs=num_epochs,\n",
    "                optimizer_params={'lr': 1e-3, 'weight_decay': 1e-4},  # Experiment with different learning rates and weight decay\n",
    "                weight_decay=1e-4,\n",
    "                max_grad_norm=1.0,  # Gradient clipping\n",
    "                monitor='pr_auc',  # Monitoring PR AUC\n",
    "                monitor_criterion='max',\n",
    "                load_best_model_at_last=True  # Load the best model at the end\n",
    "            )\n",
    "            unprotected_group = 1  # han in eth\n",
    "            sensitive_attribute_array = np.zeros(len(fold_val_dataset), dtype=int)\n",
    "            for idx, visit in enumerate(fold_val_dataset):\n",
    "                sensitive_attribute_value = visit[\"ethnicity\"][0][0]\n",
    "                if sensitive_attribute_value != unprotected_group:\n",
    "                    sensitive_attribute_array[idx] = 1\n",
    "\n",
    "            # Calculate fairness metrics for the current fold\n",
    "            # Calculate fairness metrics for the current fold\n",
    "            try:\n",
    "                fold_fairness_metrics = fairness_metrics_fn(\n",
    "                y_true,\n",
    "                y_prob,\n",
    "                sensitive_attributes=sensitive_attribute_array,\n",
    "                favorable_outcome=1,\n",
    "                metrics=None,\n",
    "                threshold=0.5,)\n",
    "                wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "                fairness_scores[\"disparate_impact\"].append(fold_fairness_metrics[\"disparate_impact\"])\n",
    "                fairness_scores[\"statistical_parity_difference\"].append(fold_fairness_metrics[\"statistical_parity_difference\"])\n",
    "                fairness_scores[\"wtpr\"].append(wtpr)\n",
    "            except:\n",
    "                print(\"DI and WTPR undefined\")\n",
    "            y_true, y_prob, loss = trainer.inference(val_loader)\n",
    "            wtpr = calculate_wtpr(y_true, y_prob, sensitive_attribute_array)\n",
    "            fairness_scores[\"wtpr\"].append(wtpr)\n",
    "\n",
    "            # Calculate F1-score\n",
    "            score = binary_metrics_fn(y_true, y_prob, metrics=[\"f1\"])\n",
    "            f1 = score['f1']\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        f1_mean = np.mean(f1_scores)\n",
    "        f1_std = np.std(f1_scores)\n",
    "        print(f\"VARIABLE DATA SIZE: {len(var_dataset)}\")\n",
    "        print(f\"F1-score: {f1_mean:.2f}±{f1_std:.2f}\")\n",
    "        for metric, scores in fairness_scores.items():\n",
    "            mean = np.nanmean(scores)  # Use nanmean to ignore NaN values\n",
    "            std = np.nanstd(scores)  # Use nanstd to ignore NaN values\n",
    "            print(f\"{metric}: {mean:.2f}±{std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b4b6d-39a5-4ae2-8136-83c0b3731ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real + FAIRSYNTH EXPERIMENT\n",
    "all_data_5000 = pickle.load(open(f\"./data/allData_pic.pkl\", \"rb\"))\n",
    "allData_1000 = all_data_5000[0:1000]\n",
    "allData_2500 = all_data_5000[0:2500]\n",
    "allData_5000 = all_data_5000[0:5000]\n",
    "\n",
    "real_datasets= [allData_1000,allData_2500,all_data_5000]\n",
    "synth_datasets= #DATA LOCATION\n",
    "fairness_acc_calc(fixed_dataset=synth_datasets,var_datasets=real_datasets,num_epochs=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
